{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f3985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import mean_squared_error # if squared=False; RMSE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "import os\n",
    "import optuna\n",
    "import pickle as pkl\n",
    "\n",
    "from joblib import Parallel, delayed # Oputna has parallelism built in but for training replicates of the selected model\n",
    "# I'll run them through Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0fb223",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './data/Processed/'\n",
    "\n",
    "phno = pd.read_csv(save_path+\"phno3.csv\")\n",
    "\n",
    "YMat = np.load(save_path+'YMat3.npy')\n",
    "GMat = np.load(save_path+'GMat3.npy')\n",
    "SMat = np.load(save_path+'SMat3.npy')\n",
    "WMat = np.load(save_path+'WMat3.npy')\n",
    "MMat = np.load(save_path+'MMat3.npy')\n",
    "\n",
    "GMatNames = np.load(save_path+'GMatNames.npy')\n",
    "SMatNames = np.load(save_path+'SMatNames.npy')\n",
    "WMatNames = np.load(save_path+'WMatNames.npy')\n",
    "MMatNames = np.load(save_path+'MMatNames.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c6c0b3",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0318eee1",
   "metadata": {},
   "source": [
    "## SKlearn modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebda163a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to panel data\n",
    "def wthr_rank_3to2(x_3d):\n",
    "    n_obs, n_days, n_metrics = x_3d.shape\n",
    "    return(x_3d.reshape(n_obs, (n_days*n_metrics)))\n",
    "\n",
    "def wthr_features_rank_2to3(x_3d, feature_import):\n",
    "    n_obs, n_days, n_metrics = x_3d.shape\n",
    "    return(feature_import.reshape(n_days, n_metrics))\n",
    "\n",
    "def y_rank_2to1(y_2d):\n",
    "    n_obs = y_2d.shape[0]\n",
    "    return(y_2d.reshape(n_obs, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd34b35",
   "metadata": {},
   "source": [
    "## Tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c809bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True == False:\n",
    "    test_year = 2016\n",
    "\n",
    "    downsample = True # FIXME\n",
    "    downsample_train = 1000\n",
    "    downsample_test  =  100\n",
    "\n",
    "    # Set up train/test indices --------------------------------------------------\n",
    "    mask_undefined = (phno.Yield_Mg_ha.isna()) # these can be imputed but not evaluated\n",
    "    mask_test = ((phno.Year == test_year) & (~mask_undefined))\n",
    "    mask_train = ((phno.Year != test_year) & (~mask_undefined))\n",
    "    test_idx = phno.loc[mask_test, ].index\n",
    "    train_idx = phno.loc[mask_train, ].index\n",
    "\n",
    "    if downsample:\n",
    "        train_idx = np.random.choice(train_idx, downsample_train)\n",
    "        test_idx = np.random.choice(test_idx, downsample_test)\n",
    "\n",
    "    # Get Scalings ---------------------------------------------------------------\n",
    "    YMat_center = np.mean(YMat[train_idx], axis = 0)\n",
    "    YMat_scale = np.std(YMat[train_idx], axis = 0)\n",
    "\n",
    "    SMat_center = np.mean(SMat[train_idx, :], axis = 0)\n",
    "    SMat_scale = np.std(SMat[train_idx, :], axis = 0)\n",
    "\n",
    "    WMat_center = np.mean(WMat[train_idx, :, :], axis = 0)\n",
    "    WMat_scale = np.std(WMat[train_idx, :, :], axis = 0)\n",
    "\n",
    "    MMat_center = np.nanmean(MMat[train_idx, :], axis = 0)\n",
    "    MMat_scale = np.nanstd(MMat[train_idx, :], axis = 0)\n",
    "    # if std is 0, set to 1\n",
    "    MMat_scale[MMat_scale == 0] = 1\n",
    "\n",
    "    # Center and Scale -----------------------------------------------------------\n",
    "    YMat = (YMat - YMat_center)/YMat_scale\n",
    "\n",
    "    SMat = (SMat - SMat_center)/SMat_scale\n",
    "\n",
    "    MMat = (MMat - MMat_center)/MMat_scale\n",
    "\n",
    "    # Split and Send to GPU ------------------------------------------------------\n",
    "    train_g = GMat[train_idx, :]\n",
    "    test_g  = GMat[test_idx, :]\n",
    "\n",
    "    train_s = SMat[train_idx, :]\n",
    "    test_s  = SMat[test_idx, :]\n",
    "\n",
    "    train_w = WMat[train_idx, :, :]\n",
    "    test_w  = WMat[test_idx, :, :]\n",
    "\n",
    "    train_m = MMat[train_idx, :]\n",
    "    test_m  = MMat[test_idx, :]\n",
    "\n",
    "\n",
    "    train_y = YMat[train_idx]\n",
    "    test_y  = YMat[test_idx]\n",
    "\n",
    "    # Reshape to rank 1\n",
    "    train_y = train_y.reshape([train_y.shape[0], 1])\n",
    "    test_y = test_y.reshape([test_y.shape[0], 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda6b3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True == False:\n",
    "    # G\n",
    "    train_x_2d = train_g\n",
    "    train_y_1d = y_rank_2to1(y_2d = train_y)\n",
    "    test_x_2d = test_g\n",
    "    test_y_1d = y_rank_2to1(y_2d = test_y)\n",
    "\n",
    "\n",
    "    regr = RandomForestRegressor(max_depth= 16, \n",
    "                                 random_state=0,\n",
    "                                 n_estimators = 20)\n",
    "    rf = regr.fit(train_x_2d, train_y_1d)\n",
    "\n",
    "\n",
    "    print([\n",
    "        mean_squared_error(train_y_1d, rf.predict(train_x_2d), squared=False), \n",
    "         mean_squared_error(test_y_1d, rf.predict(test_x_2d), squared=False)\n",
    "    ])\n",
    "\n",
    "    # px.bar(pd.DataFrame(dict(cols=GMatNames, imp=rf.feature_importances_)), x = 'cols', y = 'imp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc245e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True == False:\n",
    "    # S\n",
    "    train_x_2d = train_s\n",
    "    train_y_1d = y_rank_2to1(y_2d = train_y)\n",
    "    test_x_2d = test_s\n",
    "    test_y_1d = y_rank_2to1(y_2d = test_y)\n",
    "\n",
    "    regr = RandomForestRegressor(max_depth= 16, \n",
    "                                 random_state=0,\n",
    "                                 n_estimators = 20)\n",
    "    rf = regr.fit(train_x_2d, train_y_1d)\n",
    "\n",
    "\n",
    "    print([\n",
    "        mean_squared_error(train_y_1d, rf.predict(train_x_2d), squared=False), \n",
    "         mean_squared_error(test_y_1d, rf.predict(test_x_2d), squared=False)\n",
    "    ])\n",
    "\n",
    "    px.bar(pd.DataFrame(dict(cols=SMatNames, imp=rf.feature_importances_)), x = 'cols', y = 'imp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f931a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True == False:\n",
    "    # W\n",
    "    train_x_2d = wthr_rank_3to2(x_3d = train_w)\n",
    "    train_y_1d = y_rank_2to1(y_2d = train_y)\n",
    "\n",
    "    test_x_2d = wthr_rank_3to2(x_3d = test_w)\n",
    "    test_y_1d = y_rank_2to1(y_2d = test_y)\n",
    "    \n",
    "    regr = RandomForestRegressor(max_depth= 16, \n",
    "                                 random_state=0,\n",
    "                                 n_estimators = 20)\n",
    "    rf = regr.fit(train_x_2d, train_y_1d)\n",
    "    print([\n",
    "        mean_squared_error(train_y_1d, rf.predict(train_x_2d), squared=False), \n",
    "         mean_squared_error(test_y_1d, rf.predict(test_x_2d), squared=False)\n",
    "    ])\n",
    "\n",
    "    px.imshow(\n",
    "        wthr_features_rank_2to3(train_w, rf.feature_importances_),\n",
    "        labels=dict(y = 'cols'),\n",
    "        y = WMatNames\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90518035",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True == False:\n",
    "    # M\n",
    "    train_x_2d = train_m\n",
    "    train_y_1d = y_rank_2to1(y_2d = train_y)\n",
    "    test_x_2d = test_m\n",
    "    test_y_1d = y_rank_2to1(y_2d = test_y)\n",
    "\n",
    "    regr = RandomForestRegressor(max_depth= 16, \n",
    "                                 random_state=0,\n",
    "                                 n_estimators = 20)\n",
    "    rf = regr.fit(train_x_2d, train_y_1d)\n",
    "\n",
    "    print([\n",
    "        mean_squared_error(train_y_1d, rf.predict(train_x_2d), squared=False), \n",
    "         mean_squared_error(test_y_1d, rf.predict(test_x_2d), squared=False)\n",
    "    ])\n",
    "\n",
    "    # px.bar(pd.DataFrame(dict(cols=GMatNames, imp=rf.feature_importances_)), x = 'cols', y = 'imp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c368a021",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True == False:\n",
    "    # GSWM\n",
    "    train_x_2d = np.concatenate([train_g, train_s, wthr_rank_3to2(x_3d = train_w), train_m], axis = 1)\n",
    "    train_y_1d = y_rank_2to1(y_2d = train_y)\n",
    "    test_x_2d = np.concatenate([test_g, test_s, wthr_rank_3to2(x_3d = test_w), test_m], axis = 1)\n",
    "    test_y_1d = y_rank_2to1(y_2d = test_y)\n",
    "\n",
    "    regr = RandomForestRegressor(max_depth= 16, \n",
    "                                 random_state=0,\n",
    "                                 n_estimators = 20)\n",
    "    rf = regr.fit(train_x_2d, train_y_1d)\n",
    "\n",
    "\n",
    "    print([\n",
    "        mean_squared_error(train_y_1d, rf.predict(train_x_2d), squared=False), \n",
    "        mean_squared_error(test_y_1d, rf.predict(test_x_2d), squared=False)\n",
    "    ])\n",
    "\n",
    "    # px.bar(pd.DataFrame(dict(cols=GMatNames, imp=rf.feature_importances_)), x = 'cols', y = 'imp')\n",
    "\n",
    "    feature_imps = rf.feature_importances_\n",
    "\n",
    "    AllNames = list(GMatNames)+list(SMatNames)+list(WMatNames)+list(MMatNames)\n",
    "    AllLabels = ['G' for e in list(GMatNames)\n",
    "              ]+['S' for e in list(SMatNames)\n",
    "              ]+['W' for e in list(WMatNames)\n",
    "              ]+['M' for e in list(MMatNames)]\n",
    "\n",
    "    feature_imp_df = pd.DataFrame(\n",
    "        zip(AllNames,\n",
    "            AllLabels,\n",
    "            feature_imps), \n",
    "        columns = ['Feature', 'Group', 'Importance'])\n",
    "\n",
    "    # how important is each subset?\n",
    "    feature_imp_df.groupby(['Group']).agg(Total = ('Importance', np.sum))\n",
    "\n",
    "    # Constrain to larger effects\n",
    "    feature_imp_df.loc[feature_imp_df.Importance > 0.001, ].groupby(['Group']).agg(Total = ('Importance', np.sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f122408d",
   "metadata": {},
   "source": [
    "## Full Data Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c1f747",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = './notebook_artifacts/4_modeling_rf/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848eefa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_train_test(\n",
    "    test_this_year = '2014',\n",
    "    downsample = True, # FIXME\n",
    "    downsample_train = 1000,\n",
    "    downsample_test  =  100,\n",
    "    phno = phno,\n",
    "    GMat = GMat,\n",
    "    SMat = SMat,\n",
    "    WMat = WMat,\n",
    "    MMat = MMat,\n",
    "    YMat = YMat):\n",
    "\n",
    "    mask_undefined = (phno.Yield_Mg_ha.isna()) # these can be imputed but not evaluated\n",
    "    mask_test = ((phno.Year == int(test_this_year)) & (~mask_undefined))\n",
    "    mask_train = ((phno.Year != int(test_this_year)) & (~mask_undefined))\n",
    "    test_idx = phno.loc[mask_test, ].index\n",
    "    train_idx = phno.loc[mask_train, ].index\n",
    "\n",
    "    if downsample:\n",
    "        train_idx = np.random.choice(train_idx, downsample_train)\n",
    "        test_idx = np.random.choice(test_idx, downsample_test)\n",
    "\n",
    "\n",
    "    # Get Scalings ---------------------------------------------------------------\n",
    "    YMat_center = np.mean(YMat[train_idx], axis = 0)\n",
    "    YMat_scale = np.std(YMat[train_idx], axis = 0)\n",
    "\n",
    "    SMat_center = np.mean(SMat[train_idx, :], axis = 0)\n",
    "    SMat_scale = np.std(SMat[train_idx, :], axis = 0)\n",
    "\n",
    "    WMat_center = np.mean(WMat[train_idx, :, :], axis = 0)\n",
    "    WMat_scale = np.std(WMat[train_idx, :, :], axis = 0)\n",
    "\n",
    "    MMat_center = np.nanmean(MMat[train_idx, :], axis = 0)\n",
    "    MMat_scale = np.nanstd(MMat[train_idx, :], axis = 0)\n",
    "    # if std is 0, set to 1\n",
    "    MMat_scale[MMat_scale == 0] = 1\n",
    "\n",
    "    # Center and Scale -----------------------------------------------------------\n",
    "    YMat = (YMat - YMat_center)/YMat_scale\n",
    "    SMat = (SMat - SMat_center)/SMat_scale\n",
    "    MMat = (MMat - MMat_center)/MMat_scale\n",
    "\n",
    "    # Split ------------------------------------------------------------------\n",
    "    train_g = GMat[train_idx, :]\n",
    "    test_g  = GMat[test_idx, :]\n",
    "\n",
    "    train_s = SMat[train_idx, :]\n",
    "    test_s  = SMat[test_idx, :]\n",
    "\n",
    "    train_w = WMat[train_idx, :, :]\n",
    "    test_w  = WMat[test_idx, :, :]\n",
    "\n",
    "    train_m = MMat[train_idx, :]\n",
    "    test_m  = MMat[test_idx, :]\n",
    "\n",
    "\n",
    "    train_y = YMat[train_idx]\n",
    "    test_y  = YMat[test_idx]\n",
    "\n",
    "    # Reshape to rank 1\n",
    "    train_y = train_y.reshape([train_y.shape[0], 1])\n",
    "    test_y = test_y.reshape([test_y.shape[0], 1])\n",
    "\n",
    "    # GSWM\n",
    "    train_x_2d = np.concatenate([train_g, train_s, wthr_rank_3to2(x_3d = train_w), train_m], axis = 1)\n",
    "    train_y_1d = y_rank_2to1(y_2d = train_y)\n",
    "    test_x_2d = np.concatenate([test_g, test_s, wthr_rank_3to2(x_3d = test_w), test_m], axis = 1)\n",
    "    test_y_1d = y_rank_2to1(y_2d = test_y)\n",
    "    \n",
    "    \n",
    "    full_x_2d = np.concatenate([GMat, SMat, wthr_rank_3to2(x_3d = WMat), MMat], axis = 1)\n",
    "    return(train_x_2d, train_y_1d, test_x_2d, test_y_1d, full_x_2d, YMat_center, YMat_scale, YMat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47241355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ran from 17:15:40 2023-01-05 -> 9:15:40 2023-01-06 completing only 2014, (2015 still running single thread)\n",
    "# To reduce the run time, I'm shrinking the search space based on what was effective for 2014 and 2015\n",
    "# 2014 == *\n",
    "# 2015 == v (best so far trial 52)\n",
    "#         'rf_max_depth',    2-10v----65*----------------------200\n",
    "#      'rf_n_estimators',   20-30v-39*-------------------------500\n",
    "# 'rf_min_samples_split', 0.05-.5*v---------------------------0.95\n",
    "\n",
    "# Setup ----------------------------------------------------------------------\n",
    "trial_name = 'rf'\n",
    "# takes about 3 minutes to fit one full model\n",
    "n_trials= 120 \n",
    "n_jobs = 30\n",
    "test_this_year = '2014'\n",
    "\n",
    "downsample = False# FIXME\n",
    "downsample_train = 1000\n",
    "downsample_test  =  100\n",
    "\n",
    "def objective(trial): \n",
    "    rf_max_depth = trial.suggest_int('rf_max_depth', 2, 100, log=True)\n",
    "    rf_n_estimators = trial.suggest_int('rf_n_estimators', 20, 100, log=True)\n",
    "    rf_min_samples_split = trial.suggest_float('rf_min_samples_split', 0.005, 0.5, log=True)\n",
    "    \n",
    "    regr = RandomForestRegressor(\n",
    "        max_depth = rf_max_depth, \n",
    "        n_estimators = rf_n_estimators,\n",
    "        min_samples_split = rf_min_samples_split\n",
    "        )\n",
    "    \n",
    "    rf = regr.fit(train_x_2d, train_y_1d)\n",
    "    return (mean_squared_error(train_y_1d, rf.predict(train_x_2d), squared=False))\n",
    "\n",
    "\n",
    "if True == False:\n",
    "    reset_trial_name = trial_name\n",
    "    for test_this_year in [#'2014', '2015', '2016', '2017', '2018', '2019', '2020', \n",
    "                           '2021']:\n",
    "        print(\"\"\"\n",
    "    ------------------------------------------\n",
    "    ------------------\"\"\"+test_this_year+\"\"\"------------------\n",
    "        \"\"\")    \n",
    "\n",
    "        trial_name = reset_trial_name\n",
    "        trial_name = trial_name+test_this_year\n",
    "        print(test_this_year)\n",
    "        # Data Prep. -----------------------------------------------------------------\n",
    "        # Set up train/test indices --------------------------------------------------\n",
    "        train_x_2d, train_y_1d, test_x_2d, test_y_1d, full_x_2d, YMat_center, YMat_scale, YMat = prep_train_test(\n",
    "            test_this_year = test_this_year,\n",
    "            downsample = downsample, \n",
    "            downsample_train = downsample_train,\n",
    "            downsample_test  =  downsample_test,\n",
    "            phno = phno,\n",
    "            GMat = GMat,\n",
    "            SMat = SMat,\n",
    "            WMat = WMat,\n",
    "            MMat = MMat,\n",
    "            YMat = YMat)\n",
    "\n",
    "        # HPS Study ------------------------------------------------------------------\n",
    "        cache_save_name = cache_path+trial_name+'_hps.pkl'\n",
    "        if os.path.exists(cache_save_name):\n",
    "            study = pkl.load(open(cache_save_name, 'rb'))  \n",
    "        else:\n",
    "            study = optuna.create_study(direction=\"minimize\")\n",
    "            study.optimize(objective, n_trials= n_trials, n_jobs = n_jobs)\n",
    "            # save    \n",
    "            pkl.dump(study, open(cache_save_name, 'wb'))    \n",
    "\n",
    "        print(\"\"\"\n",
    "    --------------Study Complete--------------\n",
    "    ------------------------------------------\n",
    "        \"\"\")\n",
    "        def fit_single_rep(Rep = 1):\n",
    "            # Fit Best HPS --------------------------------------------------------------- \n",
    "            cache_save_name = cache_path+trial_name+'_'+str(Rep)+'_mod.pkl'\n",
    "\n",
    "            # Load cached model if it exists\n",
    "            if os.path.exists(cache_save_name):\n",
    "                rf = pkl.load(open(cache_save_name, 'rb'))  \n",
    "            else:\n",
    "                regr = RandomForestRegressor(\n",
    "                        max_depth = study.best_trial.params['rf_max_depth'], \n",
    "                        n_estimators = study.best_trial.params['rf_n_estimators'],\n",
    "                        min_samples_split = study.best_trial.params['rf_min_samples_split']\n",
    "                        )\n",
    "                rf = regr.fit(train_x_2d, train_y_1d)\n",
    "                # save    \n",
    "                pkl.dump(rf, open(cache_save_name, 'wb'))   \n",
    "\n",
    "            # Record Predictions -----------------------------------------------------\n",
    "            out = phno.copy()\n",
    "            out['YHat'] = rf.predict(full_x_2d)\n",
    "            out['YMat'] = YMat\n",
    "            out['Y_center'] = YMat_center\n",
    "            out['Y_scale'] = YMat_scale\n",
    "            out['Class'] = trial_name\n",
    "            out['CV'] = test_this_year\n",
    "            out['Rep'] = Rep\n",
    "    #         out.to_csv('./notebook_artifacts/4_modeling_rf/'+trial_name+'_'+str(Rep)+'rfYHats.csv')\n",
    "            out.to_csv('./data/Shared_Model_Output/'+trial_name+'_'+str(Rep)+'rfYHats.csv')\n",
    "\n",
    "        # use joblib to get replicate models all at once\n",
    "        Parallel(n_jobs=10)(delayed(fit_single_rep)(Rep = i) for i in range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282fad18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [I 2023-01-06 08:56:55,012] Trial 27 finished with value: 0.7037294697990369 and parameters: {'rf_max_depth': 33, 'rf_n_estimators': 266, 'rf_min_samples_split': 0.05867248086866843}. Best is trial 52 with value: 0.6961002467654831."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,auto:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
