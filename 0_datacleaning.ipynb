{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09620871",
   "metadata": {},
   "source": [
    "# Process Data G2F Competition Data. \n",
    "\n",
    "> This notebook is the first pass at cleaning the aggregated g2f data. It will draw on code written for past projects, specifically `maizemodel` and `g2fd`. I will also use random forests to help identify which the highest value targets for data cleaning are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47aa44d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, requests # for downloading power data with `dl_power_data()`\n",
    "import glob\n",
    "import re\n",
    "import time # add a delay between requests to NASA POWER API for weather data\n",
    "import pickle as pkl\n",
    "\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from sklearn.impute import KNNImputer # for imputing soil for ARH1_2016 & ARH2_2016\n",
    "from sklearn import preprocessing # LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error # if squared=False; RMSE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import optuna\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# from g2f_comp.internal import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220b5e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = './notebook_artifacts/0_datacleaning/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61babcb5",
   "metadata": {},
   "source": [
    "# Data Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755afce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir= \"./data/Maize_GxE_Competition_Data/Training_Data/\"\n",
    "test_dir = \"./data/Maize_GxE_Competition_Data/Testing_Data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f3c9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handy one liners for finding mismatched data types\n",
    "# find disagreeing types\n",
    "# [e for e in [e for e in list(x_test) if e in list(x_train)] if (np.dtype(x_train[e]) != np.dtype(x_test[e])) ]\n",
    "# [(e, np.dtype(x_train[e]), np.dtype(x_test[e])) for e in [e for e in list(x_test) if e in list(x_train)] if (np.dtype(x_train[e]) != np.dtype(x_test[e])) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64826e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate train/test into a single set of dfs\n",
    "# phenotype/trait\n",
    "x_train = pd.read_csv(train_dir+'1_Training_Trait_Data_2014_2021.csv')\n",
    "x_test = pd.read_csv(test_dir+'1_Submission_Template_2022.csv')\n",
    "# use env to fill in other keys\n",
    "# x_test[[\"Field_Location\", \"Year\"]] = x_test['Env'].str.split('_',expand=True)\n",
    "# x_test[\"Year\"] = x_test[\"Year\"].astype(int)\n",
    "phno = x_train.merge(x_test, \"outer\")\n",
    "\n",
    "\n",
    "# metadata\n",
    "# I've manually resaved this csv as an xlsx to get around this error with the csv:\n",
    "# UnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 5254: invalid continuation byte\n",
    "x_train = pd.read_excel(train_dir+'2_Training_Meta_Data_2014_2021.xlsx') \n",
    "x_test = pd.read_csv(test_dir+'2_Testing_Meta_Data_2022.csv')\n",
    "x_test['Date_weather_station_placed'] = pd.to_datetime(x_test.Date_weather_station_placed)\n",
    "x_test['Date_weather_station_removed'] = pd.to_datetime(x_test.Date_weather_station_removed)\n",
    "# to string\n",
    "for e in ['Weather_Station_Serial_Number (Last four digits, e.g. m2700s#####)',\n",
    "         'Issue/comment_#5', \n",
    "          'Issue/comment_#6', \n",
    "          'Comments']:\n",
    "    x_test[e] = x_test[e].astype(str)\n",
    "meta = x_train.merge(x_test, how = \"outer\")\n",
    "\n",
    "\n",
    "# soil\n",
    "x_train = pd.read_csv(train_dir+'3_Training_Soil_Data_2015_2021.csv')\n",
    "x_test = pd.read_csv(test_dir+'3_Testing_Soil_Data_2022.csv')\n",
    "\n",
    "for e in ['E Depth',\n",
    " 'lbs N/A',\n",
    " 'Potassium ppm K',\n",
    " 'Calcium ppm Ca',\n",
    " 'Magnesium ppm Mg',\n",
    " 'Sodium ppm Na',\n",
    " '%H Sat',\n",
    " '%K Sat',\n",
    " '%Ca Sat',\n",
    " '%Mg Sat',\n",
    " '%Na Sat',\n",
    " 'Mehlich P-III ppm P']:\n",
    "    x_test[e] = x_test[e].astype(float)\n",
    "\n",
    "x_test['Comments'] = x_test['Comments'].astype(str)\n",
    "soil = x_train.merge(x_test, how = \"outer\")\n",
    "\n",
    "\n",
    "# weather\n",
    "x_train = pd.read_csv(train_dir+'4_Training_Weather_Data_2014_2021.csv')\n",
    "x_test = pd.read_csv(test_dir+'4_Testing_Weather_Data_2022.csv')\n",
    "wthr = x_train.merge(x_test, how = \"outer\")\n",
    "\n",
    "# crop growth model variables (enviromental covariates)\n",
    "x_train = pd.read_csv(train_dir+'6_Training_EC_Data_2014_2021.csv')\n",
    "x_test = pd.read_csv(test_dir+'6_Testing_EC_Data_2022.csv')\n",
    "cgmv = x_train.merge(x_test, how = \"outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6b06db",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "## Data Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29d6a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from g2fd\n",
    "def find_shared_cols(\n",
    "    df1,# = df,\n",
    "    df2# = meta\n",
    "):\n",
    "    shared_cols = [e for e in list(df1) if e in list(df2)]\n",
    "    return(shared_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670b1543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes involving only 1 data frame =======================================\n",
    "# phno\n",
    "# object to datetime\n",
    "phno.Date_Planted = pd.to_datetime(phno.Date_Planted)\n",
    "phno.Date_Harvested = pd.to_datetime(phno.Date_Harvested)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ee6f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to make sure there are no missing values for these\n",
    "assert [] == [e for e in list(cgmv) if 0 != np.mean(cgmv[e].isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ce6681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes involving _2_ data frames =========================================\n",
    "\n",
    "# phno and meta share 'Plot_Area_ha', 'Date_Planted' these need to be checked for consistency and moved. \n",
    "# use meta to fill in pheno then drop\n",
    "# meta has the information from 22 and nothing else. Reverse for phno\n",
    "for e in ['Plot_Area_ha', 'Date_Planted']:\n",
    "    mask = phno[e].isna()\n",
    "    fill_ins = phno.loc[mask, ['Env', 'Year']].drop_duplicates().reset_index()\n",
    "\n",
    "    # for all the unique key combinations find the value that should be inserted and do so.\n",
    "    for i in range(fill_ins.shape[0]):\n",
    "        phno_mask = ((phno.Env == fill_ins.loc[i,\"Env\"]) & \n",
    "                    (phno.Year == fill_ins.loc[i,\"Year\"]) )\n",
    "\n",
    "        meta_mask = ((meta.Env == fill_ins.loc[i,\"Env\"]) & \n",
    "                    (meta.Year == fill_ins.loc[i,\"Year\"]) )\n",
    "        \n",
    "\n",
    "        \n",
    "        insert_val = list(meta.loc[meta_mask, e])\n",
    "        insert_val = [e for e in insert_val if e == e] # get rid of nans \n",
    "        \n",
    "        if insert_val == []:\n",
    "            pass\n",
    "        else:\n",
    "            assert len(insert_val) == 1 # check that there's only one value to imput\n",
    "            phno.loc[phno_mask, e] = insert_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71314dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = meta.drop(columns=['Plot_Area_ha', 'Date_Planted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29003e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from g2fd\n",
    "# generalized version of `sanitize_Experiment_Codes`\n",
    "def sanitize_col(df, col, simple_renames= {}, split_renames= {}):\n",
    "    # simple renames\n",
    "    for e in simple_renames.keys():\n",
    "        mask = (df[col] == e)\n",
    "        df.loc[mask, col] = simple_renames[e]\n",
    "\n",
    "    # splits\n",
    "    # pull out the relevant multiname rows, copy, rename, append\n",
    "    for e in split_renames.keys():\n",
    "        mask = (df[col] == e)\n",
    "        temp = df.loc[mask, :] \n",
    "\n",
    "        df = df.loc[~mask, :]\n",
    "        for e2 in split_renames[e]:\n",
    "            temp2 = temp.copy()\n",
    "            temp2[col] = e2\n",
    "            df = df.merge(temp2, how = 'outer')\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5dc445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_col_missing(df):\n",
    "    return(\n",
    "        pd.DataFrame({'Col'   : [e for e in list(df)],\n",
    "              'N_miss' : [sum(df[e].isna()) for e in list(df)],\n",
    "              'Pr_Comp': [round(100*(1-sum(df[e].isna())/len(df[e])), 1) for e in list(df)]})\n",
    "    )\n",
    "# summarize_col_missing(df = meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b98436e",
   "metadata": {},
   "source": [
    "## Misc. Column Rearranging and Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b62917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_Envs = ['GEH1_2020', 'GEH1_2021', 'GEH1_2019' # Germany locations\n",
    "#           'ARH1_2016', 'ARH2_2016' # Only in 2016, missing soil data\n",
    "          ]\n",
    "\n",
    "phno = phno.loc[~phno.Env.isin(rm_Envs), :]\n",
    "meta = meta.loc[~meta.Env.isin(rm_Envs), :]\n",
    "soil = soil.loc[~soil.Env.isin(rm_Envs), :]\n",
    "wthr = wthr.loc[~wthr.Env.isin(rm_Envs), :]\n",
    "cgmv = cgmv.loc[~cgmv.Env.isin(rm_Envs), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8c6579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix types: object to datetime\n",
    "phno.Date_Planted = pd.to_datetime(phno.Date_Planted)\n",
    "phno.Date_Harvested = pd.to_datetime(phno.Date_Harvested)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f18a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop and redo year and Env\n",
    "phno = phno.drop(columns = ['Year'])\n",
    "meta = meta.drop(columns = ['Year'])\n",
    "soil = soil.drop(columns = ['Year'])\n",
    "\n",
    "\n",
    "temp = pd.DataFrame(phno['Env']).drop_duplicates().reset_index().drop(columns = 'index')\n",
    "temp['Env2'] = temp['Env']\n",
    "\n",
    "\n",
    "temp = sanitize_col(\n",
    "    df = temp, \n",
    "    col = 'Env2', \n",
    "    simple_renames= {\n",
    "    'MOH1_1_2018': 'MOH1-1_2018', \n",
    "    'MOH1_2_2018': 'MOH1-2_2018', \n",
    "    'MOH1_1_2020': 'MOH1-1_2020', \n",
    "    'MOH1_2_2020': 'MOH1-2_2020'\n",
    "    }, \n",
    "    split_renames= {})\n",
    "\n",
    "\n",
    "assert [] == [e for e in list(temp['Env2']) if len(e.split('_')) != 2]\n",
    "temp[[\"Experiment_Code\", \"Year\"]] = temp['Env2'].str.split('_',expand=True)\n",
    "temp = temp.drop(columns = ['Experiment_Code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c9f566",
   "metadata": {},
   "outputs": [],
   "source": [
    "phno = temp.merge(phno).drop(columns = ['Env']).rename(columns = {'Env2':'Env'})\n",
    "meta = temp.merge(meta).drop(columns = ['Env']).rename(columns = {'Env2':'Env'})\n",
    "soil = temp.merge(soil).drop(columns = ['Env']).rename(columns = {'Env2':'Env'})\n",
    "wthr = temp.merge(wthr).drop(columns = ['Env']).rename(columns = {'Env2':'Env'})\n",
    "cgmv = temp.merge(cgmv).drop(columns = ['Env']).rename(columns = {'Env2':'Env'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6330ebc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "(find_shared_cols(phno, meta),\n",
    " find_shared_cols(phno, soil),\n",
    " find_shared_cols(phno, wthr),\n",
    " find_shared_cols(phno, cgmv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62540175",
   "metadata": {},
   "source": [
    "## Add in missing Enviroments\n",
    "confirm there are envs/gps coordinates for everyone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e34c4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "phno_Envs = list(set(phno['Env']))\n",
    "meta_Envs = list(set(meta['Env']))\n",
    "soil_Envs = list(set(soil['Env']))\n",
    "wthr_Envs = list(set(wthr['Env']))\n",
    "cgmv_Envs = list(set(cgmv['Env']))\n",
    "\n",
    "all__Envs = list(set(phno_Envs+soil_Envs+wthr_Envs+cgmv_Envs))\n",
    "\n",
    "def get_missing_envs(data_Envs = []):\n",
    "    return([e for e in all__Envs if e not in data_Envs])\n",
    "\n",
    "phno_Envs_miss = get_missing_envs(data_Envs = phno_Envs)\n",
    "meta_Envs_miss = get_missing_envs(data_Envs = meta_Envs)\n",
    "soil_Envs_miss = get_missing_envs(data_Envs = soil_Envs)\n",
    "wthr_Envs_miss = get_missing_envs(data_Envs = wthr_Envs)\n",
    "cgmv_Envs_miss = get_missing_envs(data_Envs = cgmv_Envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad9ee81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out to be imputed envs / wholely absent envs\n",
    "if [] != phno_Envs_miss:\n",
    "    pd.DataFrame({'Absent_Envs':phno_Envs_miss}).to_csv('./data/Preparation/phno_Envs_miss.csv', index=False)\n",
    "if [] != meta_Envs_miss:\n",
    "    pd.DataFrame({'Absent_Envs':meta_Envs_miss}).to_csv('./data/Preparation/meta_Envs_miss.csv', index=False)\n",
    "if [] != soil_Envs_miss:\n",
    "    pd.DataFrame({'Absent_Envs':soil_Envs_miss}).to_csv('./data/Preparation/soil_Envs_miss.csv', index=False)\n",
    "if [] != wthr_Envs_miss:\n",
    "    pd.DataFrame({'Absent_Envs':wthr_Envs_miss}).to_csv('./data/Preparation/wthr_Envs_miss.csv', index=False)\n",
    "if [] != cgmv_Envs_miss:\n",
    "    pd.DataFrame({'Absent_Envs':cgmv_Envs_miss}).to_csv('./data/Preparation/cgmv_Envs_miss.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54e3b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert [] == phno_Envs_miss\n",
    "assert [] == meta_Envs_miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358df80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_envs = phno.loc[(phno.Env.isin(soil_Envs_miss)), ['Env', 'Year']].drop_duplicates()\n",
    "soil = soil.merge(add_envs, how = 'outer')\n",
    "soil_Envs_miss = get_missing_envs(data_Envs = list(set(soil['Env'])))\n",
    "assert [] == soil_Envs_miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005cae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_envs = phno.loc[(phno.Env.isin(wthr_Envs_miss)), ['Env', 'Year']].drop_duplicates()\n",
    "add_envs = add_envs.merge(\n",
    "    wthr.loc[(wthr.Year.isin(list(set(add_envs['Year'])))), ['Year', 'Date']].drop_duplicates(),\n",
    "    how = 'outer'\n",
    ")\n",
    "wthr = wthr.merge(add_envs, how = 'outer')\n",
    "wthr_Envs_miss = get_missing_envs(data_Envs = list(set(wthr['Env'])))\n",
    "assert [] == wthr_Envs_miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a465faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_envs = phno.loc[(phno.Env.isin(cgmv_Envs_miss)), ['Env', 'Year']].drop_duplicates()\n",
    "cgmv = cgmv.merge(add_envs, how = 'outer')\n",
    "cgmv_Envs_miss = get_missing_envs(data_Envs = list(set(cgmv['Env'])))\n",
    "assert [] == cgmv_Envs_miss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8f1ff3",
   "metadata": {},
   "source": [
    "## Reorganize columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7760497e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regroup phenotype and meta Columns\n",
    "df = phno.merge(meta)\n",
    "\n",
    "cols_for_phno = [\n",
    "    'Env',\n",
    "    'Year',\n",
    "    'Hybrid',\n",
    "    'Yield_Mg_ha',          #| <- Key target\n",
    "    'Stand_Count_plants',   #|- Possible intermediate prediction targets\n",
    "    'Pollen_DAP_days',      #|\n",
    "    'Silk_DAP_days',        #|\n",
    "    'Plant_Height_cm',      #|\n",
    "    'Ear_Height_cm',        #|\n",
    "    'Root_Lodging_plants',  #|\n",
    "    'Stalk_Lodging_plants', #|\n",
    "    'Grain_Moisture',       #|\n",
    "    'Twt_kg_m3']            #|\n",
    "\n",
    "cols_for_meta = [\n",
    "    'Env',\n",
    "    'Year',\n",
    "    #  'Field_Location',\n",
    "    #  'Experiment',\n",
    "    #  'Replicate',\n",
    "    #  'Block',\n",
    "    #  'Plot',\n",
    "    #  'Range',\n",
    "    #  'Pass',\n",
    "    'Hybrid',\n",
    "    #  'Hybrid_orig_name',\n",
    "    #  'Hybrid_Parent1',\n",
    "    #  'Hybrid_Parent2',\n",
    "    'Plot_Area_ha',     #|- Needs Imputation\n",
    "    'Date_Planted',     #|\n",
    "    'Date_Harvested',   #|\n",
    "    'Experiment_Code',\n",
    "    'Treatment',\n",
    "    'City',\n",
    "    'Farm',\n",
    "    'Field',\n",
    "    'Trial_ID (Assigned by collaborator for internal reference)', # \n",
    "    'Soil_Taxonomic_ID and horizon description, if known',        # 32.2% complete\n",
    "    'Weather_Station_Serial_Number (Last four digits, e.g. m2700s#####)',\n",
    "    'Weather_Station_Latitude (in decimal numbers NOT DMS)',\n",
    "    'Weather_Station_Longitude (in decimal numbers NOT DMS)',\n",
    "    #  'Date_weather_station_placed',\n",
    "    #  'Date_weather_station_removed',\n",
    "    'Previous_Crop',\n",
    "    'Pre-plant_tillage_method(s)',\n",
    "    'In-season_tillage_method(s)',\n",
    "    'Type_of_planter (fluted cone; belt cone; air planter)',\n",
    "    'System_Determining_Moisture',\n",
    "    'Pounds_Needed_Soil_Moisture',\n",
    "    'Latitude_of_Field_Corner_#1 (lower left)',\n",
    "    'Longitude_of_Field_Corner_#1 (lower left)',\n",
    "    'Latitude_of_Field_Corner_#2 (lower right)',\n",
    "    'Longitude_of_Field_Corner_#2 (lower right)',\n",
    "    'Latitude_of_Field_Corner_#3 (upper right)',\n",
    "    'Longitude_of_Field_Corner_#3 (upper right)',\n",
    "    'Latitude_of_Field_Corner_#4 (upper left)',\n",
    "    'Longitude_of_Field_Corner_#4 (upper left)',\n",
    "    'Cardinal_Heading_Pass_1',\n",
    "    'Irrigated']\n",
    "\n",
    "cols_for_cmnt =[\n",
    "    'Env',\n",
    "    'Year',\n",
    "    'Issue/comment_#1',\n",
    "    'Issue/comment_#2',\n",
    "    'Issue/comment_#3',\n",
    "    'Issue/comment_#4',\n",
    "    'Issue/comment_#5',\n",
    "    'Issue/comment_#6',\n",
    "    'Comments']\n",
    "\n",
    "phno = df.loc[:, cols_for_phno]\n",
    "meta = df.loc[:, cols_for_meta]\n",
    "cmnt = df.loc[:, cols_for_cmnt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846b2ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with Issues/Comments\n",
    "# I think I'll ignore these fields. There's qualitative information we can get\n",
    "# and additional weather data (below) but not a lot that will be immediately \n",
    "# useful\n",
    "cmnt = cmnt.drop_duplicates()\n",
    "cmnt = cmnt.melt(id_vars = ['Env', 'Year'])\n",
    "cmnt = cmnt.loc[(cmnt.value.notna()), :]\n",
    "cmnt = cmnt.loc[(cmnt.value != 'nan'), :]\n",
    "\n",
    "cmnts_2022  = list(set(list(cmnt.loc[cmnt.Year == \"2022\", 'value'])))\n",
    "# cmnts_2022\n",
    "\n",
    "\n",
    "# looking at comments from 2022 we could make use of some of these if we were \n",
    "# making manual guesses but these won't be too helpful here.\n",
    "# - 'Some very low yields caused by greensnap\\n',\n",
    "# - 'Deer damage on the early hybrids',\n",
    "# - 'sandhill cranes pulled out seedlings in sections of the field in late May/early June',\n",
    "# - 'Thunderstorm caused very high level of root lodging and some greensnap.  Corn was upright at end of season, but mostly goosenecked.\\nRoot lodging not recroded becuase of high percentage of goosenecking\\nGreensnap not counted separately, so stalk lodging includes both goosenecking and stalk lodging\\nSome very low yields caused by greensnap\\n',\n",
    "\n",
    "# However, the additional weather may be of use.\n",
    "# TODO, get additional weather data\n",
    "# 'Link to additional weather source available online: http://www.deos.udel.edu',\n",
    "# 'Link to additional weather source available online: https://weather.cfaes.osu.edu//stationinfo.asp?id=13',\n",
    "# 'Link to additional weather source available online: Georgia Weather - Automated Environmental Monitoring Network Page (uga.edu) - http://weather.uga.edu/mindex.php?content=calculator&variable=CC&site=TIFTON',\n",
    "# 'Link to additional weather source available online: https://newa.cornell.edu/all-weather-data-query/   (select Aurora (CUAES Musgrave), NY)',\n",
    "# 'Link to additional weather source available online: https://www.isws.illinois.edu/warm/stationmeta.asp?site=CMI&from=wx'\n",
    "# more from pre 2022\n",
    "# 'http://newa.cornell.edu/index.php?page=weather-station-page&WeatherStation=aur',\n",
    "# 'Additional weather source available online: http://www.georgiaweather.net/?content=calculator&variable=CC&site=PENFIELD',\n",
    "# 'http://www.deos.udel.edu',\n",
    "# 'Additional weather data source: http://www.deos.udel.edu',\n",
    "# ' https://soilseries.sc.egov.usda.gov/OSD_Docs/M/MEXICO.html',\n",
    "# 'Link to additional weahter source: http://www.deos.udel.edu',\n",
    "# 'Link to additional weahter source: Georgia Weather - Automated Environmental Monitoring Network Page (uga.edu) http://weather.uga.edu/mindex.php?variable=HI&site=TIFTON',\n",
    "# 'Additional weather data for the farm can be found at: http://newa.cornell.edu/index.php?page=weather-station-page&WeatherStation=aur',\n",
    "# 'Addicional weather source available online: http://agebb.missouri.edu/weather/history/index.asp?station_prefix=bfd',\n",
    "# 'http://weather.uga.edu/mindex.php?content=calculator&variable=CC&site=TIFTON',\n",
    "\n",
    "\n",
    "# [e for e in list(set(list(cmnt.loc[cmnt.Year != \"2022\", 'value']))) if e not in cmnts_2022]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89948ff4",
   "metadata": {},
   "source": [
    "## Meta (1/2) Impute GPS Coordinates\n",
    "\n",
    " Fix Missing GPS Coordinates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d91fea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix GPS coordinates\n",
    "gps_cols = ['Latitude_of_Field_Corner_#1 (lower left)',\n",
    "'Latitude_of_Field_Corner_#2 (lower right)',\n",
    "'Latitude_of_Field_Corner_#3 (upper right)',\n",
    "'Latitude_of_Field_Corner_#4 (upper left)',\n",
    "            \n",
    "'Longitude_of_Field_Corner_#1 (lower left)',\n",
    "'Longitude_of_Field_Corner_#2 (lower right)',\n",
    "'Longitude_of_Field_Corner_#3 (upper right)',\n",
    "'Longitude_of_Field_Corner_#4 (upper left)',\n",
    "            \n",
    "'Weather_Station_Latitude (in decimal numbers NOT DMS)',\n",
    "'Weather_Station_Longitude (in decimal numbers NOT DMS)']\n",
    "\n",
    "\n",
    "gps = meta.loc[:, ['Env']+gps_cols]\n",
    "\n",
    "\n",
    "# exclude sites outside of north america (side benefit of disqualifying wrongly input data from TXH1_2021)\n",
    "# Logitude must be \n",
    "longitude_max = -60\n",
    "latitude_max = 45\n",
    "\n",
    "for col in gps_cols:\n",
    "    print(\"Cleaning: '\"+col+\"'\")\n",
    "    if re.match('.*Latitude.+', col):\n",
    "        mask = gps[col ]>latitude_max\n",
    "    elif re.match('.*Longitude.+', col):\n",
    "        mask = gps[col ]>longitude_max\n",
    "    print(\"Erasing values in: '\"+\"', '\".join(list(set(list(gps.loc[mask, 'Env']))))+\"'\")    \n",
    "    gps.loc[mask, col] = np.nan\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dab40da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# melt and get field center\n",
    "gps = gps.melt(id_vars=['Env'])\n",
    "\n",
    "mask = gps.variable.isin([\n",
    "    'Latitude_of_Field_Corner_#1 (lower left)',\n",
    "    'Latitude_of_Field_Corner_#2 (lower right)',\n",
    "    'Latitude_of_Field_Corner_#3 (upper right)',\n",
    "    'Latitude_of_Field_Corner_#4 (upper left)'\n",
    "                  ])\n",
    "\n",
    "gps.loc[mask, 'variable'] = 'Latitude_of_Field'\n",
    "\n",
    "mask = gps.variable.isin([\n",
    "    'Longitude_of_Field_Corner_#1 (lower left)',\n",
    "    'Longitude_of_Field_Corner_#2 (lower right)',\n",
    "    'Longitude_of_Field_Corner_#3 (upper right)',\n",
    "    'Longitude_of_Field_Corner_#4 (upper left)'\n",
    "                  ])\n",
    "\n",
    "gps.loc[mask, 'variable'] = 'Longitude_of_Field'\n",
    "\n",
    "# collapse measures for the field\n",
    "gps = gps.groupby(['Env', 'variable']).agg(value = ('value', np.nanmean)).reset_index()\n",
    "\n",
    "# if we have more accurate information, don't factor the weather station location into the estimate of the field location\n",
    "gps['Replace'] = False\n",
    "gps.loc[gps.value.isna(), 'Replace'] = True\n",
    "\n",
    "mask = ((~(gps.Replace)) & (gps.variable == 'Weather_Station_Latitude (in decimal numbers NOT DMS)'))\n",
    "gps.loc[mask, 'value'] = np.nan\n",
    "\n",
    "mask = ((~(gps.Replace)) & (gps.variable == 'Weather_Station_Longitude (in decimal numbers NOT DMS)'))\n",
    "gps.loc[mask, 'value'] = np.nan\n",
    "\n",
    "gps = gps.drop(columns = ['Replace'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86b901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat the same trick to use the weather station info if the field info is not known\n",
    "mask = gps.variable.isin([\n",
    "    'Weather_Station_Latitude (in decimal numbers NOT DMS)',\n",
    "    'Latitude_of_Field'\n",
    "                  ])\n",
    "\n",
    "gps.loc[mask, 'variable'] = 'Latitude_of_Field'\n",
    "\n",
    "mask = gps.variable.isin([\n",
    "    'Weather_Station_Longitude (in decimal numbers NOT DMS)',\n",
    "    'Longitude_of_Field'\n",
    "                  ])\n",
    "\n",
    "gps.loc[mask, 'variable'] = 'Longitude_of_Field'\n",
    "\n",
    "gps = gps.groupby(['Env', 'variable']).agg(value = ('value', np.nanmean)).reset_index()\n",
    "\n",
    "gps = gps.pivot(index='Env', columns='variable', values='value').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da3ccf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some envs have no gps info, impute those based on similarly named locations\n",
    "\n",
    "# There is only one TXH4 group (TXH4_2019), so the below approach doesn't work. \n",
    "# I'll use all the TXH* sites to guess the coordinates.\n",
    "mask = [True if re.match(\"TXH.+\", e) else False for e in gps['Env']]\n",
    "\n",
    "gps.loc[(gps.Env == 'TXH4_2019'), 'Latitude_of_Field'] = np.nanmedian(gps.loc[mask, 'Latitude_of_Field'])\n",
    "gps.loc[(gps.Env == 'TXH4_2019'), 'Longitude_of_Field'] = np.nanmedian(gps.loc[mask, 'Longitude_of_Field'])\n",
    "\n",
    "# Impute all non-TXH4 locations\n",
    "gps[['EnvBase', 'EnvYear']] = gps.Env.str.split(\"_\", expand = True)\n",
    "\n",
    "mask_no_lat = gps.Latitude_of_Field.isna()\n",
    "mask_no_lon = gps.Longitude_of_Field.isna()\n",
    "\n",
    "impute_gps_vals = list(gps.loc[(mask_no_lat | mask_no_lon), 'Env'])\n",
    "\n",
    "\n",
    "# for each Env with missing values, use the first portion of the name\n",
    "# e.g. TXH1-Early_2017 -> TXH1 to search for possible matches\n",
    "for impute_gps_val in impute_gps_vals:\n",
    "    mask_gps_val = (gps.Env == impute_gps_val)\n",
    "    match_root = gps.loc[mask_gps_val, 'EnvBase'].str.split('-')\n",
    "    match_root = list(match_root)[0][0]\n",
    "    #     print()\n",
    "\n",
    "    mask = [True if re.match(e, match_root+'.+') else False for e in gps['EnvBase']]\n",
    "\n",
    "    check_std_lat = round(np.nanstd(gps.loc[mask, 'Latitude_of_Field']), 3)\n",
    "    check_std_lon = round(np.nanstd(gps.loc[mask, 'Longitude_of_Field']),3)\n",
    "\n",
    "    gps.loc[mask_gps_val, 'Latitude_of_Field']  = np.nanmedian(gps.loc[mask, 'Latitude_of_Field'])\n",
    "    gps.loc[mask_gps_val, 'Longitude_of_Field'] = np.nanmedian(gps.loc[mask, 'Longitude_of_Field'])\n",
    "\n",
    "    \n",
    "gps = gps.drop(columns = ['EnvBase', 'EnvYear'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3216a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sum(gps.Latitude_of_Field.isna()) == 0\n",
    "assert sum(gps.Longitude_of_Field.isna()) == 0\n",
    "print(\"All GPS Coordinates Imputed!\")    \n",
    "meta = meta.drop(columns = gps_cols).merge(gps)\n",
    "print(\"And replaced in `meta`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb02e4f",
   "metadata": {},
   "source": [
    "## Mock up CV Groupings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c2a62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(phno.Hybrid)\n",
    "obs_summary = phno.loc[:, ['Env', 'Year', 'Hybrid', 'Yield_Mg_ha']].merge(meta.loc[:, ['Env', 'Latitude_of_Field', 'Longitude_of_Field']].drop_duplicates())\n",
    "\n",
    "distance_threshold = 1 \n",
    "\n",
    "# Create clusters with lat/lon\n",
    "temp = obs_summary.loc[:, ['Env', 'Latitude_of_Field', 'Longitude_of_Field']\n",
    "                      ].drop_duplicates(\n",
    "                      ).reset_index(\n",
    "                      ).drop(columns = 'index')\n",
    "temp['GPS_Group'] = \"\"\n",
    "temp['Distance'] = np.nan\n",
    "\n",
    "gps_group_counter = 0\n",
    "\n",
    "for i in temp.index:\n",
    "    match_lat = temp.loc[i, 'Latitude_of_Field'] \n",
    "    match_lon = temp.loc[i, 'Longitude_of_Field'] \n",
    "\n",
    "    if temp.loc[i, 'GPS_Group'] == '':\n",
    "        temp['Distance'] = np.sqrt( (temp['Latitude_of_Field'] - float(match_lat))**2 + (temp['Longitude_of_Field']  - float(match_lon))**2 )\n",
    "\n",
    "        temp.loc[(temp.Distance <= distance_threshold), 'GPS_Group'] = str(gps_group_counter)\n",
    "        gps_group_counter += 1\n",
    "\n",
    "\n",
    "obs_summary = obs_summary.merge(temp.loc[:, ['Env', 'Latitude_of_Field', 'Longitude_of_Field', 'GPS_Group']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030427b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gps_groups_2022 = list(set(obs_summary.loc[(obs_summary.Year == '2022'), 'GPS_Group']))\n",
    "mask_pre_2022 = (obs_summary.Year != '2022')\n",
    "mask_gps_match = (obs_summary.GPS_Group.isin(gps_groups_2022))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80405fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_obs_summary_tally(df = obs_summary.loc[:, ['Year', 'Env']]):\n",
    "    df = df.groupby(['Year']\n",
    "                   ).count(\n",
    "                   ).reset_index(\n",
    "                   ).assign(\n",
    "        Pct   = lambda dataframe: round(dataframe['Env']/np.sum(dataframe['Env']), 4)*100,\n",
    "        Total = lambda dataframe: np.sum(dataframe['Env']))\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c8283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: the whole dataset\n",
    "quick_obs_summary_tally(df = obs_summary.loc[:, ['Year', 'Env']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d99436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The full possible testing dataset\n",
    "obs_pre_2022 = quick_obs_summary_tally(df = obs_summary.loc[mask_pre_2022, ['Year', 'Env']])\n",
    "obs_pre_2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3585d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The testing dataset constrained to gps groups in 2022\n",
    "obs_pre_2022_gps = quick_obs_summary_tally(df = obs_summary.loc[(mask_pre_2022 & mask_gps_match), ['Year', 'Env']])\n",
    "\n",
    "obs_diff = obs_pre_2022_gps.loc[0, 'Total'] - obs_pre_2022.loc[0, 'Total']\n",
    "obs_pct = 100*round(obs_diff/obs_pre_2022.loc[0, 'Total'], 4)\n",
    "print(str(obs_diff)+' fewer obs\\n'+str(obs_pct)+'% fewer obs')\n",
    "obs_pre_2022_gps['Diff'] = obs_pre_2022_gps['Env']\n",
    "obs_pre_2022_gps['Diff'] = obs_pre_2022_gps['Diff'] - obs_pre_2022['Env']\n",
    "\n",
    "obs_pre_2022_gps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90bd0de",
   "metadata": {},
   "source": [
    "This doesn't remove too many observations from recent years but it's almost equivalent to a whole year. I think I'll not restrict the training set to only matching gps groups.\n",
    "\n",
    "Because I'll be ensembling models, I need multiple testing sets for\n",
    "1. Hyperparameter selection\n",
    "1.  \n",
    "1. \n",
    "\n",
    "```\n",
    "Hyperparameter |-> Training |-> Ensemble |-> Predictions\n",
    "Selection      |            |   Tuning   |\n",
    "- 1 year       |            |            |\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df35e054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools # for making test set permutations\n",
    "testing_years = pd.DataFrame(\n",
    "    [e for e in itertools.permutations([2014+i for i in range(8)], 3)], \n",
    "    columns = ['Test_HPS', 'Test_Model', 'Test_Ensemble'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bbea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(2039476435238045723476)\n",
    "testing_years['Random_Order'] = rng.permutation(testing_years.shape[0])\n",
    "testing_years.sort_values('Random_Order')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843cae26",
   "metadata": {},
   "source": [
    "## Soil Drop Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08f1946",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(soil)\n",
    "\n",
    "summarize_col_missing(soil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62665b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop low completion rate entries\n",
    "temp = summarize_col_missing(df = soil)\n",
    "# high percent complete columns\n",
    "high_pr_comp_cols = list(temp.loc[(temp.Pr_Comp) > 50, # this threshold used to be 70, but the additon of envs \n",
    "                                  # which did not have soil measured deflate the completion rates\n",
    "                                  'Col'])\n",
    "\n",
    "soil = soil.loc[:, high_pr_comp_cols].drop(columns = [\n",
    "    'LabID',           #|- Not interested in these columns\n",
    "    'Date Received',   #|\n",
    "    'Date Reported'])  #|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b1d8c3",
   "metadata": {},
   "source": [
    "## Meta Drop Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bfa3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out a log of the enviroments imputed\n",
    "def log_imputed_envs(\n",
    "    df = meta,\n",
    "    df_name = 'meta',\n",
    "    col = 'Date_Planted'\n",
    "):\n",
    "    mask = df[col].isna()\n",
    "    df = df.loc[mask, ['Env']].drop_duplicates().reset_index().drop(columns = ['index'])\n",
    "    df.to_csv('./data/Preparation/'+df_name+'_Envs_imp_'+col+'.csv', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7bb358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discard columns that have low completion or redundant information\n",
    "meta = meta.drop(columns = [e for e in list(meta) if e in [\n",
    "    'Experiment_Code',\n",
    "    'Treatment',\n",
    "    'City',\n",
    "    'Farm',\n",
    "    'Field',\n",
    "    'Trial_ID (Assigned by collaborator for internal reference)',\n",
    "    'Soil_Taxonomic_ID and horizon description, if known',\n",
    "    'Weather_Station_Serial_Number (Last four digits, e.g. m2700s#####)',\n",
    "    'Type_of_planter (fluted cone; belt cone; air planter)',\n",
    "    'In-season_tillage_method(s)', # 34% Pr_Comp\n",
    "    'Plot_Area_ha', # 92.1 % complete but not a covariate I want to use\n",
    "    'System_Determining_Moisture',\n",
    "    'Cardinal_Heading_Pass_1',\n",
    "    'Irrigated']])\n",
    "\n",
    "\n",
    "summarize_col_missing(meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb6da20",
   "metadata": {},
   "source": [
    "### Previous Crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c8a08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_imputed_envs(\n",
    "    df = meta,\n",
    "    df_name = 'meta',\n",
    "    col = 'Previous_Crop'\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2152ac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need data dicts before I can encode\n",
    "Previous_Crop = {\n",
    "                                 'soybean': 'soy',\n",
    "                                  'cotton': 'cotton',\n",
    "                                   'wheat': 'wheat',\n",
    "               'wheat/double crop soybean': 'soy_wheat',\n",
    "                                    'corn': 'corn',\n",
    "   'Lima beans followed by rye cover crop': 'lima_rye',\n",
    "                                 'sorghum': 'sorghum',\n",
    "                            'Winter wheat': 'wheat',\n",
    "   'Fallow most of 2014 winter planted in fall of 2014 then sprayed with Glystar 24 floz/a on 5/3/15  and killed spring of 2015 spray ': 'fallow',\n",
    "                                  'peanut': 'peanut',\n",
    "           'wheat and Double Crop soybean': 'soy_wheat',\n",
    "                              'sugar beet': 'beet',\n",
    "    'Small Grains and Double Crop soybean': 'soy_rye',\n",
    "                         'soybean/pumpkin': 'soy_pumpkin',\n",
    "                           'wheat/soybean': 'soy_wheat',\n",
    "     'soybeans with fall cereal rye cover': 'soy_rye'\n",
    "}\n",
    "set('_'.join(list(set([Previous_Crop[e] for e in Previous_Crop.keys()]))).split('_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435023b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame(\n",
    "    zip([e for e in Previous_Crop.keys()],\n",
    "        [Previous_Crop[e] for e in Previous_Crop.keys()]), \n",
    "    columns = ['Previous_Crop', 'Value'])\n",
    "\n",
    "for crop in ['beet', 'corn', 'cotton', 'fallow', 'lima', \n",
    "             'peanut', 'pumpkin', 'rye', 'sorghum', 'soy', \n",
    "             'wheat']:\n",
    "    temp['Cover_'+crop]  = [1 if re.search(crop, e) else 0 for e in temp['Value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100edf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = meta.merge(temp, how = 'outer').drop(columns = ['Value'])\n",
    "meta = meta.drop(columns = 'Previous_Crop')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cbfe9b",
   "metadata": {},
   "source": [
    "### Pre-plant_tillage_method(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574c3939",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_imputed_envs(\n",
    "    df = meta,\n",
    "    df_name = 'meta',\n",
    "    col = 'Pre-plant_tillage_method(s)'\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d107a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pre_plant_tillage_method = {\n",
    "    'Conventional': 'Cult',\n",
    "    'Disc in previous fall': 'Disc',\n",
    "    'conventional': 'Cult',\n",
    "    'field cultivator': 'Cult',\n",
    "    'Fall Chisel': 'Chisel',\n",
    "    'Fall chisel plow and spring field cultivate': 'Chisel_Cult',\n",
    "    'chisel': 'Chisel',\n",
    "    'No-till': 'None',\n",
    "    'Chisel plow and field cultivator': 'Chisel_Cult',\n",
    "    'chisel plow in fall; field cultivated in spring': 'Chisel_Cult',\n",
    "    'In the Spring the land was cut with a disk, then ripped with a chisel plow to a depth of 8-10”. It was then cut again and we applied 300#/acre of 10-0-30-12%S. Next we used a field cultivator with rolling baskets to incorporate the fertilizer. The land was bedded just prior to planting.': 'Chisel_Cult_Disc',\n",
    "    'no-till': 'None',\n",
    "    'Field J was fall moldboard plow;  Then disked this spring and field cultivated before planting.': 'Chisel_Cult_Disc',\n",
    "    'The field was minium tilled.  The field was disked then cultipacked then Cultimulched then planted': 'Cult_Disc_MinTill',\n",
    "    'Fall Chisel Plow; Spring Cultivate': 'Chisel_Cult',\n",
    "    'min-till': 'MinTill',\n",
    "    'Field cultivator': 'Cult',\n",
    "    'Field cultivate': 'Cult',\n",
    "    'No-Till': 'None',\n",
    "    'fall chisel plow, spring field cultivator': 'Chisel_Cult',\n",
    "    'disc, conventional, followed by bedding': 'Disc_Cult',\n",
    "    'No Till': 'None',\n",
    "    'Chisel plowed 5/4/15 Disc and finishing tool 5/6/15 ': 'Chisel_Disc',\n",
    "    'Chisel plowed 5/7/15 disc and field finisher 5/23/15': 'Chisel_Disc',\n",
    "    'Fall Chisel, Spring Turbo-Till': 'Chisel_Cult',\n",
    "    'Min-Till': 'MinTill',\n",
    "    'Cultivate, hip and row': 'Cult',\n",
    "    'Conventional disc tillage': 'Disc_Cult',\n",
    "    'Field cultivated': 'Cult',\n",
    "    'Chisel Plow': 'Chisel',\n",
    "    'Conventional Tillage': 'Cult',\n",
    "    'Moldboard plowed November\\n': 'Chisel',\n",
    "    'Fall Diskchisel, Spring Culivator': 'Disc_Cult',\n",
    "    'Cultivator ': 'Cult',\n",
    "    'Cultivate': 'Cult',\n",
    "    'Min-Till ': 'MinTill',\n",
    "    'Field Cultivator': 'Cult',\n",
    "    'cultivate, hip, row': 'Cult',\n",
    "    'none': 'None',\n",
    "    'Chisel': 'Chisel',\n",
    "    'Fall plow/Spring field cultivator': 'Chisel_Cult',\n",
    "    'disk and hip': 'Disc',\n",
    "    'till and hip': 'Chisel',\n",
    "    '1 pass with soil finisher': 'Cult',\n",
    "    'disked, chisel plow and field cultivator': 'Chisel_Disc_Cult',\n",
    "    'harrowed, rototilled': 'Cult',\n",
    "    'disc': 'Disc',\n",
    "    'Two passes with disk, one pass with field conditioner, 30Ó beds were made with 8 row ripper-bedder for corn': 'Disc_Cult',\n",
    "    'Fall Diskchisel / Spring Culivator': 'Chisel_Cult',\n",
    "    'chisel plow': 'Chisel',\n",
    "    '2 passes with a field cultivator': 'Cult',\n",
    "    'disk': 'Disc',\n",
    "    'Chisel plow,cultivate': 'Chisel_Cult',\n",
    "    'Fall soil chisel, spring cultivator': 'Chisel_Cult',\n",
    "    'field cultivate': 'Cult',\n",
    "    'cultivate, hip and row': 'Cult',\n",
    "    'disked, ripped, field cultivator': 'Disc_Cult',\n",
    "    'Ripper Bed, rototill': 'Chisel',\n",
    "    'standard (disk plow)': 'Disc_Chisel_Cult',\n",
    "    'spring field cultivator': 'Cult',\n",
    "    'fall disk chisel, spring cultivator': 'Chisel_Cult',\n",
    "    'chisel, field cultivate': 'Chisel_Cult',\n",
    "    'Chisel plow followed by  cultivator': 'Chisel_Cult',\n",
    "    'field cultivate (twice)': 'Cult',\n",
    "    'Chisel plow': 'Chisel',\n",
    "    'Chisel plowed on 12/14/17': 'Chisel',\n",
    "    'Heavy disk, Chisel plow, Field cultivator': 'Chisel_Disc_Cult',\n",
    "    'Disked, chisil, disk': 'Chisel_Disc_Cult',\n",
    "    'Ripper Bed, Rototill': 'Cult',\n",
    "    'disked and field conditioned': 'Disc',\n",
    "    'field cultivate ': 'Cult',\n",
    "    'Case IH 335 VT 4\" deep ': 'Cult',\n",
    "    'Chisel - field cultivator': 'Cult',\n",
    "    'Chisel plow followed by cultivator': 'Chisel_Cult',\n",
    "    'CaseIH VT 360 vertical tillage tool gone over 2X on 5/22': 'Cult',\n",
    "    'CaseIH VT 360 vertical tillage tool gone over 2X on 5/23': 'Cult',\n",
    "    'Fall Diskchisel, Spring Disk': 'Disc',\n",
    "    'Cultivator': 'Cult',\n",
    "    'disc harrow followed by chisel plow, field cultivator used to prepare final seed bed': 'Disc_Chisel_Cult',\n",
    "    'harrow, ripper bed, rototill': 'Cult',\n",
    "    'soil finisher': 'None',\n",
    "    'conventional, heavy disc, chisel plow, field cultivator': 'Disc_Chisel_Cult',\n",
    "    'cultivate 2x': 'Cult',\n",
    "    'cultivator': 'Cult',\n",
    "    'Conventional tillage (disc) + ripped and bedded rows': 'Disc',\n",
    "    'Heavy Disk and then rows placed': 'Disc',\n",
    "    'Chisel plow, disk, field cultivator': 'Disc_Chisel_Cult',\n",
    "    'Harrow, ripper bed, rototill': 'Cult',\n",
    "    'Field cultivator. Tillage with soil finisher': 'Cult',\n",
    "    'Strip tillage': 'Cult',\n",
    "    'Fall rip one pass tool, spring field cultivate': 'Cult',\n",
    "    'conventional - Field Cultivator, disk': 'Cult',\n",
    "    'None': 'None',\n",
    "    'CaseIH VT 360 vertical tillage tool gone over 2X on 5/18/2021': 'Cult',\n",
    "    'CaseIH VT 360 vertical tillage tool gone over 2X on 5/18': 'Cult',\n",
    "    'Conventional, heavy disc, chisel plow, field cultivator': 'Disc_Chisel_Cult',\n",
    "    'Disked the whole field and then rows were placed': 'Disc',\n",
    "    'Disc, Dynadrive': 'Disc',\n",
    "    'ripper bed, rototill': 'Cult',\n",
    "    'Disk': 'Disc',\n",
    "    'Disk, field cultivator, ripper/bedder': 'Disc_Chisel_Cult',\n",
    "    'Fall disk chisel, Spring cultivator': 'Disc_Chisel_Cult',\n",
    "    'conventional, heavy disc, chisel plow, and field cultivator': 'Disc_Chisel_Cult',\n",
    "    'Discing': 'Disc_Chisel_Cult'\n",
    "}\n",
    "\n",
    "set('_'.join(list(set([Pre_plant_tillage_method[e] for e in Pre_plant_tillage_method.keys()]))).split('_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4f9053",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame(\n",
    "    zip([e for e in Pre_plant_tillage_method.keys()],\n",
    "        [Pre_plant_tillage_method[e] for e in Pre_plant_tillage_method.keys()]), \n",
    "    columns = ['Pre-plant_tillage_method(s)', 'Value'])\n",
    "\n",
    "temp['Pre_Chisel']  = [1 if re.search('Chisel', e) else 0 for e in temp['Value']]\n",
    "temp['Pre_Cult']    = [1 if re.search('Cult', e) else 0 for e in temp['Value']]\n",
    "temp['Pre_Disc']    = [1 if re.search('Disc', e) else 0 for e in temp['Value']]\n",
    "temp['Pre_MinTill'] = [1 if re.search('MinTill', e) else 0 for e in temp['Value']]\n",
    "\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0af95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = meta.merge(temp, how = 'outer').drop(columns = ['Pre-plant_tillage_method(s)', 'Value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed537576",
   "metadata": {},
   "source": [
    "### Pounds_Needed_Soil_Moisture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128dff20",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_imputed_envs(\n",
    "    df = meta,\n",
    "    df_name = 'meta',\n",
    "    col = 'Pounds_Needed_Soil_Moisture'\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e7d146",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = sanitize_col(df = meta, col = 'Pounds_Needed_Soil_Moisture', \n",
    "             simple_renames= {\n",
    "                 'Unknown': '-9999',\n",
    "                 'Unknown, currently getting in contact with manufacturer, technician estimated about 5 lbs of grain to get moisture reading':'5',\n",
    " '4 or 5':'4.5',\n",
    " '~2.5':'2.5',\n",
    " \"Based on the technitian's experience a minimum of 4 lbs is required. However the user manual says the minimum volume for accurate determination is 2 liters\":'4',\n",
    " 'Depend on moisture content of grain 15.5% moisture 5.84 lbs 30% moisture 7.05 lbs':'6.445',\n",
    " '2.5 lbs.':'2.5',\n",
    " '5-6.5':'5.75',\n",
    " '3 to 4':'3.5',\n",
    " '~5 lbs':'5',\n",
    " '~10 lbs':'10',\n",
    " '7 to 9':'8',\n",
    " '<1': '0.5'\n",
    "             }, \n",
    "             split_renames= {})\n",
    "\n",
    "meta['Pounds_Needed_Soil_Moisture'] =  meta.loc[:, 'Pounds_Needed_Soil_Moisture'].astype(float)\n",
    "\n",
    "mask = meta['Pounds_Needed_Soil_Moisture'] == -9999\n",
    "meta.loc[mask, 'Pounds_Needed_Soil_Moisture'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977ecf57",
   "metadata": {},
   "source": [
    "### Numerical imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cee19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_plant_cols = [\n",
    "    'Pre_Chisel',\n",
    "    'Pre_Cult',\n",
    "    'Pre_Disc',\n",
    "    'Pre_MinTill']\n",
    "\n",
    "cover_crop_cols = [\n",
    "    'Cover_beet',\n",
    "    'Cover_corn',\n",
    "    'Cover_cotton',\n",
    "    'Cover_fallow',\n",
    "    'Cover_lima',\n",
    "    'Cover_peanut',\n",
    "    'Cover_pumpkin',\n",
    "    'Cover_rye',\n",
    "    'Cover_sorghum',\n",
    "    'Cover_soy',\n",
    "    'Cover_wheat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f09cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode impute the one hot encoded variables\n",
    "from scipy import stats # for stats.mode for imputation\n",
    "\n",
    "for col in pre_plant_cols+cover_crop_cols:\n",
    "    temp = meta.loc[:, ['Env', col]].drop_duplicates()\n",
    "    imp_val = stats.mode(temp[col], keepdims = False).mode\n",
    "    mask = meta[col].isna()\n",
    "    meta.loc[mask, col] = imp_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d60b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# median impute soil moisture\n",
    "mask = meta['Pounds_Needed_Soil_Moisture'].isna()\n",
    "\n",
    "meta.loc[mask, 'Pounds_Needed_Soil_Moisture'] = np.nanmedian((meta['Pounds_Needed_Soil_Moisture']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7d81a4",
   "metadata": {},
   "source": [
    "## Soil Impute Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce180ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute soil based on GPS coordinates\n",
    "# Use distance between lon/lat to fill in missing values with the nearest. \n",
    "# I'm using euclidean distance of lon/lat so this is not perfect.\n",
    "temp = meta.loc[:, ['Env', 'Latitude_of_Field', 'Longitude_of_Field']].drop_duplicates()\n",
    "\n",
    "soil = soil.merge(temp).drop(columns = 'Texture') # with sand silt clay percents this is redundant info\n",
    "\n",
    "check_cols = [e for e in list(soil) if e not in ['Env', 'Year', 'Latitude_of_Field', 'Longitude_of_Field']]\n",
    "\n",
    "for check_col in check_cols:\n",
    "    # check_col = check_cols[0]\n",
    "    mask = soil[check_col].isna()\n",
    "\n",
    "    fill_envs = soil.loc[mask, 'Env']\n",
    "    for fill_env in fill_envs:\n",
    "        # fill_env = fill_envs[0]\n",
    "        \n",
    "        if np.dtype(soil[check_col]) != 'O':\n",
    "            match_lat = soil.loc[(soil.Env == fill_env),  'Latitude_of_Field'] \n",
    "            match_lon = soil.loc[(soil.Env == fill_env),  'Longitude_of_Field'] \n",
    "\n",
    "            soil['Distance'] = np.sqrt( ((soil['Latitude_of_Field']  - float(match_lat))**2\n",
    "                                    ) + ((soil['Longitude_of_Field'] - float(match_lon))**2))\n",
    "\n",
    "            dist_min = np.nanmin(soil.loc[(soil.Env != fill_env), 'Distance'])\n",
    "\n",
    "#             print(dist_min)\n",
    "            dist_mask = soil.Distance == dist_min\n",
    "\n",
    "            soil.loc[(soil.Env == fill_env), check_col] = np.nanmedian(soil.loc[dist_mask, check_col])\n",
    "\n",
    "soil = soil.drop(columns = ['Latitude_of_Field', 'Longitude_of_Field', 'Distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad8a38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute soil without GPS coordinates\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "knn_imputed = pd.DataFrame(\n",
    "    imputer.fit_transform(\n",
    "        soil.drop(columns = ['Env', 'Year'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cd6ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_imputed.columns = [e for e in list(soil) if e not in ['Env', 'Year']]\n",
    "\n",
    "soil = pd.concat([soil.loc[:, ['Env', 'Year']], knn_imputed], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386e459c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on WSL this was fine, but on linux it is asserting an error desipte there being none below 100.0. Below works just fine.\n",
    "# assert False not in (summarize_col_missing(soil).loc[:, 'Pr_Comp'] == 100) \n",
    "# Below works as expected.\n",
    "assert False not in [True if e == 100 else False for e in list(summarize_col_missing(soil).loc[:, 'Pr_Comp'])]\n",
    "print(\"No missing values in `soil`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126935a3",
   "metadata": {},
   "source": [
    "## Weather Impute Missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fac5452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_power_data(\n",
    "    latitude = 32.929, \n",
    "    longitude = -95.770,\n",
    "    start_YYYYMMDD = 20150101,\n",
    "    end_YYYYMMDD = 20150305\n",
    "):\n",
    "    # Modified by \n",
    "    # https://power.larc.nasa.gov/docs/tutorials/service-data-request/api/\n",
    "    '''\n",
    "    *Version: 2.0 Published: 2021/03/09* Source: [NASA POWER](https://power.larc.nasa.gov/)\n",
    "    POWER API Multi-Point Download\n",
    "    This is an overview of the process to request data from multiple data points from the POWER API.\n",
    "    '''\n",
    "\n",
    "    base_url = r\"https://power.larc.nasa.gov/api/temporal/daily/point?parameters=QV2M,T2MDEW,PS,RH2M,WS2M,GWETTOP,ALLSKY_SFC_SW_DWN,ALLSKY_SFC_PAR_TOT,T2M_MAX,T2M_MIN,T2MWET,GWETROOT,T2M,GWETPROF,ALLSKY_SFC_SW_DNI,PRECTOTCORR&community=RE&longitude={longitude}&latitude={latitude}&start={start_YYYYMMDD}&end={end_YYYYMMDD}&format=JSON\"\n",
    "\n",
    "    api_request_url = base_url.format(\n",
    "        longitude=longitude, \n",
    "        latitude=latitude,\n",
    "        start_YYYYMMDD=start_YYYYMMDD, \n",
    "        end_YYYYMMDD=end_YYYYMMDD)\n",
    "\n",
    "    response = requests.get(url=api_request_url, verify=True, timeout=30.00)\n",
    "\n",
    "    content = json.loads(response.content.decode('utf-8'))\n",
    "\n",
    "    # Repackage content as data frame\n",
    "    df_list = [\n",
    "        pd.DataFrame(content['properties']['parameter'][e], index = [0]).melt(\n",
    "        ).rename(columns = {'variable':'Date', 'value':e})\n",
    "        for e in list(content['properties']['parameter'].keys())\n",
    "    ]\n",
    "\n",
    "    for i in range(len(df_list)):\n",
    "        if i == 0:\n",
    "            out = df_list[i]\n",
    "        else:\n",
    "            out = out.merge(df_list[i])\n",
    "\n",
    "    out['Latitude'] = latitude\n",
    "    out['Longitude'] = longitude\n",
    "    first_cols = ['Latitude', 'Longitude', 'Date']\n",
    "    out = out.loc[:, first_cols+[e for e in list(out) if e not in first_cols]]\n",
    "    return(out)\n",
    "\n",
    "\n",
    "# dl_power_data(\n",
    "#     latitude = 32.929, \n",
    "#     longitude = -95.770,\n",
    "#     start_YYYYMMDD = 20150101,\n",
    "#     end_YYYYMMDD = 20150305\n",
    "# )\n",
    "\n",
    "# for reference, here's some info on the structure of contents\n",
    "# dict_keys(['type', 'geometry', 'properties', 'header', 'messages', 'parameters', 'times'])\n",
    "# content['header']\n",
    "# {'title': 'NASA/POWER CERES/MERRA2 Native Resolution Daily Data',\n",
    "#  'api': {'version': 'v2.3.5', 'name': 'POWER Daily API'},\n",
    "#  'sources': ['power', 'ceres', 'merra2'],\n",
    "#  'fill_value': -999.0,\n",
    "#  'start': '20150101',\n",
    "#  'end': '20150305'}\n",
    "# content['parameters']\n",
    "# {'QV2M': {'units': 'g/kg', 'longname': 'Specific Humidity at 2 Meters'},\n",
    "#  'T2MDEW': {'units': 'C', 'longname': 'Dew/Frost Point at 2 Meters'},\n",
    "#  'PS': {'units': 'kPa', 'longname': 'Surface Pressure'},\n",
    "#  'RH2M': {'units': '%', 'longname': 'Relative Humidity at 2 Meters'},\n",
    "#  'WS2M': {'units': 'm/s', 'longname': 'Wind Speed at 2 Meters'},\n",
    "#  'GWETTOP': {'units': '1', 'longname': 'Surface Soil Wetness'},\n",
    "#  'ALLSKY_SFC_SW_DWN': {'units': 'kW-hr/m^2/day',\n",
    "#   'longname': 'All Sky Surface Shortwave Downward Irradiance'},\n",
    "#  'ALLSKY_SFC_PAR_TOT': {'units': 'W/m^2',\n",
    "#   'longname': 'All Sky Surface PAR Total'},\n",
    "#  'T2M_MAX': {'units': 'C', 'longname': 'Temperature at 2 Meters Maximum'},\n",
    "#  'T2M_MIN': {'units': 'C', 'longname': 'Temperature at 2 Meters Minimum'},\n",
    "#  'T2MWET': {'units': 'C', 'longname': 'Wet Bulb Temperature at 2 Meters'},\n",
    "#  'GWETROOT': {'units': '1', 'longname': 'Root Zone Soil Wetness'},\n",
    "#  'T2M': {'units': 'C', 'longname': 'Temperature at 2 Meters'},\n",
    "#  'GWETPROF': {'units': '1', 'longname': 'Profile Soil Moisture'},\n",
    "#  'ALLSKY_SFC_SW_DNI': {'units': 'kW-hr/m^2/day',\n",
    "#   'longname': 'All Sky Surface Shortwave Downward Direct Normal Irradiance'},\n",
    "#  'PRECTOTCORR': {'units': 'mm/day', 'longname': 'Precipitation Corrected'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2787720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this shows that the downloaded values and given values are almost in perfect\n",
    "# agreement (ALLSKY_SFC_SW_DWN, ALLSKY_SFC_SW_DNI) are the only discrepencies \n",
    "# for the test case. I'll list out those sites with errors and selectively \n",
    "# download them and fill in missing values. To be polite, check if the weather\n",
    "# data already exists before drawing from POWER's API.\n",
    "\n",
    "# NOTE. Most, but not all of the missing values are due to the release date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0e2176",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True == False:\n",
    "    wthr_DEH1_2014 = wthr.loc[wthr.Env == 'DEH1_2014', :]\n",
    "    lat, lon = list(meta.loc[meta.Env == 'DEH1_2014', ['Latitude_of_Field', 'Longitude_of_Field']].loc[0, :])\n",
    "\n",
    "    powr_DEH1_2014 = dl_power_data(\n",
    "        latitude = lat, \n",
    "        longitude = lon,\n",
    "        start_YYYYMMDD = np.min(wthr_DEH1_2014.Date),\n",
    "        end_YYYYMMDD = np.max(wthr_DEH1_2014.Date)\n",
    "    )\n",
    "\n",
    "    pd.DataFrame(\n",
    "        [(e, (np.mean(wthr_DEH1_2014[e] - powr_DEH1_2014[e]))\n",
    "         ) for e in list(wthr_DEH1_2014) if e not in ['Env', 'Year', 'Date']\n",
    "        ], columns = ['Measure', 'Total_Difference']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8556639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "polite_request_interval = 10 # time in seconds\n",
    "\n",
    "# check if clean weather exists, load if it does, download and fill in if not\n",
    "\n",
    "if 'wthr_powr_imp.csv' in os.listdir('./data/Preparation/'):\n",
    "    print('Reading Weather from Preparation/')\n",
    "    wthr = pd.read_csv('./data/Preparation/wthr_powr_imp.csv')\n",
    "else:\n",
    "    print('Filling in Weather from NASA Power')\n",
    "    # Figure out what Envs need to be imputed\n",
    "    temp = wthr\n",
    "\n",
    "    temp = temp.drop(columns = ['Year', 'Date'])\n",
    "    for col in [e for e in list(temp) if e != 'Env']:\n",
    "        temp[col] = temp[col].isna()\n",
    "    temp = temp.drop_duplicates()\n",
    "\n",
    "    temp['Num_Rep'] = temp.drop(columns = ['Env']).sum(axis = 1)\n",
    "    impute_envs = list(temp.loc[(temp.Num_Rep > 0), 'Env'].drop_duplicates())\n",
    "    # impute_envs\n",
    "    \n",
    "    \n",
    "    for impute_env in impute_envs:\n",
    "        # impute_env = 'WIH3_2022'#impute_envs[0]\n",
    "\n",
    "        meta_mask = (meta.Env == impute_env)\n",
    "        lat = float(meta.loc[meta_mask, 'Latitude_of_Field'].drop_duplicates())\n",
    "        lon = float(meta.loc[meta_mask, 'Longitude_of_Field'].drop_duplicates())\n",
    "\n",
    "        wthr_mask = (wthr.Env == impute_env)\n",
    "\n",
    "        date_min = np.min(wthr.loc[wthr_mask, 'Date'])\n",
    "        date_max = np.max(wthr.loc[wthr_mask, 'Date'])\n",
    "\n",
    "        # Delay between requests \n",
    "        if impute_env != impute_envs[0]:\n",
    "            time.sleep(polite_request_interval)\n",
    "\n",
    "        powr_dl = dl_power_data(\n",
    "            latitude = lat, \n",
    "            longitude = lon,\n",
    "            start_YYYYMMDD = date_min,\n",
    "            end_YYYYMMDD = date_max\n",
    "        )\n",
    "\n",
    "        # cut out the previous download and add in the next\n",
    "        # change type to allow for merging\n",
    "        powr_dl['Date'] = powr_dl['Date'].astype(int)\n",
    "        wthr_slice = wthr.loc[wthr_mask, ['Env', 'Year', 'Date']].merge(powr_dl)\n",
    "        wthr = wthr.loc[~wthr_mask, :].merge(wthr_slice, how = 'outer')\n",
    "        \n",
    "    wthr = wthr.drop(columns = ['Latitude', 'Longitude'])\n",
    "    wthr.to_csv('./data/Preparation/wthr_powr_imp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbca3a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if clean weather exists, load if it does, download and fill in if not\n",
    "if 'wthr_powr_imp_knn.csv' in os.listdir('./data/Preparation/'):\n",
    "    wthr_knn = pd.read_csv('./data/Preparation/wthr_powr_imp_knn.csv')\n",
    "else:    \n",
    "    # For values that are missing in POWER (-999), remove and then knn impute them\n",
    "    for col in [e for e in list(wthr) if sum((wthr[e] == -999)) > 0 ]:\n",
    "        mask = (wthr[col] == -999)\n",
    "        wthr.loc[mask, col] = np.nan    \n",
    "    \n",
    "    \n",
    "    imputer = KNNImputer(n_neighbors=20)\n",
    "    knn_imputed = pd.DataFrame(\n",
    "        imputer.fit_transform(\n",
    "            wthr.drop(columns = ['Env', 'Year', 'Date'])))\n",
    "\n",
    "    knn_imputed.columns = [e for e in list(wthr) if e not in ['Env', 'Year', 'Date']]\n",
    "\n",
    "    wthr_knn = pd.concat([wthr.loc[:, ['Env', 'Year', 'Date']], knn_imputed], axis = 1)\n",
    "    wthr_knn.to_csv('./data/Preparation/wthr_powr_imp_knn.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53939217",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clip to last doy\n",
    "wthr = wthr_knn\n",
    "\n",
    "temp = wthr.loc[:, ['Year', 'Date']].drop_duplicates()\n",
    "temp['Date_Str'] = temp['Date'].astype(str)\n",
    "temp['DOY'] = [pd.Period(e, freq='D').day_of_year for e in list(temp['Date'])]\n",
    "\n",
    "\n",
    "# 2022 is the constraint. It only goes up to day 314. \n",
    "clip_doy = np.max(temp.loc[(temp.Year == 2022), 'DOY'])\n",
    "\n",
    "temp = temp.loc[(temp.DOY <= clip_doy), ]\n",
    "temp = temp.drop(columns = ['Date_Str'])\n",
    "\n",
    "wthr = wthr.merge(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb7a0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same problem as above. Unclear why. \n",
    "# assert False not in (summarize_col_missing(wthr).loc[:, 'Pr_Comp'] == 100)\n",
    "assert False not in [True if e == 100 else False for e in list(summarize_col_missing(wthr).loc[:, 'Pr_Comp'])]\n",
    "print(\"No missing values in `wthr`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1654e8cb",
   "metadata": {},
   "source": [
    "## CGMV Impute Missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c880ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_imputed_envs(\n",
    "    df = cgmv,\n",
    "    df_name = 'cgmv',\n",
    "    col = 'SDR_pGerEme_1'\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750df1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = cgmv.SDR_pGerEme_1.isna()\n",
    "imp_Envs = cgmv.loc[mask, ['Env', ]].drop_duplicates()\n",
    "imp_Envs = imp_Envs.reset_index().drop(columns = 'index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b52cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "gps_lookup = meta.loc[:, ['Env', 'Latitude_of_Field', 'Longitude_of_Field']].drop_duplicates()\n",
    "# get rid of sites that need to be imputed in cgmv so the closest value is good to go. \n",
    "antimask = gps_lookup.Env.isin(imp_Envs['Env'])\n",
    "gps_search = gps_lookup.loc[antimask, ]\n",
    "gps_lookup = gps_lookup.loc[~antimask, ]\n",
    "gps_lookup['Distance'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665db4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_find_closes(Env):\n",
    "    mask = (gps_search.Env == Env)\n",
    "    lat = list(gps_search.loc[mask, ]['Latitude_of_Field'])[0]\n",
    "    lon = list(gps_search.loc[mask, ]['Longitude_of_Field'])[0]\n",
    "\n",
    "    gps_lookup['Distance'] = np.sqrt( ((gps_lookup['Latitude_of_Field']  - float(lat))**2\n",
    "                                        ) + ((gps_lookup['Longitude_of_Field'] - float(lon))**2))\n",
    "\n",
    "    out = gps_lookup.loc[(gps_lookup.Distance == min(gps_lookup.Distance)), 'Env']\n",
    "    return(list(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0ee361",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_with_Envs = [temp_find_closes(e) for e in list(imp_Envs.Env)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d009c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is very sloppy but it's effective and fast to write\n",
    "# for all the envs that need to be matched, get the closest Env(s) then loop\n",
    "# over the cols\n",
    "for i in tqdm.tqdm(range(len(list(imp_Envs.Env)))):\n",
    "    fillin = list(imp_Envs.Env)[i]\n",
    "    fillinwith = imp_with_Envs[i]\n",
    "\n",
    "    mask_fillin = cgmv.Env == fillin\n",
    "    mask_fillinwith = cgmv.Env.isin(fillinwith)\n",
    "\n",
    "    for col in [e for e in list(cgmv) if e not in ['Env', 'Year']]:\n",
    "        fillin_value = np.nanmean(cgmv.loc[mask_fillinwith, col])\n",
    "        cgmv.loc[mask_fillin, col] = fillin_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96037365",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_col_missing(cgmv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aab78f",
   "metadata": {},
   "source": [
    "# Test Model Workflow: Impute Missing Management Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e244b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out the workflow in miniature by imputing Planting date\n",
    "mask = (testing_years.Random_Order == 66)\n",
    "# Test_HPS, Test_Model, Test_Ensemble =\n",
    "holdout_years = list(\n",
    "    testing_years.loc[mask, [\n",
    "        'Test_HPS', \n",
    "        'Test_Model', \n",
    "        'Test_Ensemble']].reset_index().drop(columns = 'index').loc[0, :])\n",
    "\n",
    "holdout_years = holdout_years + [2022]\n",
    "holdout_years = [str(e) for e in holdout_years]\n",
    "\n",
    "\n",
    "# 1. Hyperparameters =====\n",
    "hps_hold = holdout_years[-4:]\n",
    "hps_test = holdout_years[0:1]\n",
    "\n",
    "# 2. Model =====\n",
    "mod_hold = holdout_years[-3:]\n",
    "mod_test = holdout_years[1:2]\n",
    "\n",
    "# 3. Ensemble =====\n",
    "ens_hold = holdout_years[-2:]\n",
    "ens_test = holdout_years[2:3]\n",
    "\n",
    "# 4. Submission =====\n",
    "sub_hold = holdout_years[-1:]\n",
    "sub_test = holdout_years[3:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f50aca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the input data \n",
    "class df_prep():\n",
    "    def __init__(self):\n",
    "        self.train = {\n",
    "            \"set\":None,\n",
    "              \"x\":None,\n",
    "              \"y\":None,\n",
    "            \"yna\":None\n",
    "        }    \n",
    "        self.test = {\n",
    "            \"set\":None,\n",
    "              \"x\":None,\n",
    "              \"y\":None,\n",
    "            \"yna\":None\n",
    "        }  \n",
    "        self.cs_dict = None\n",
    "        self.isolate_missing_y_run = False # This is just for a guard rail in mk_scale_dict\n",
    "       \n",
    "    # set up the dfs for the y var\n",
    "    def get_train_test_Envs(\n",
    "        self,\n",
    "        df = meta,\n",
    "        holdout_years = ['2020', '2014', '2016', '2022'],\n",
    "        test_year = ['2020']\n",
    "        ):\n",
    "            mask = df.Year.isin(holdout_years+test_year)\n",
    "            train_set = df.loc[~mask, ['Env', 'Year']].drop_duplicates()\n",
    "\n",
    "            mask = df.Year.isin(test_year)\n",
    "            test_set = df.loc[mask, ['Env', 'Year']].drop_duplicates()\n",
    "            \n",
    "            self.train['set'] = train_set\n",
    "            self.test['set'] = test_set\n",
    "\n",
    "    ## Retrieve data based on Envs in test/train sets ==========================\n",
    "    # add in y variable\n",
    "    def _mk_ys_df(\n",
    "        self,\n",
    "        df_envs, #= train_HPS, # self. ['set']\n",
    "        df_data = meta,\n",
    "        add_cols = ['Date_Planted']\n",
    "        ):\n",
    "        df_data = df_data.loc[:, ['Env']+add_cols]\n",
    "        df_out = df_envs.merge(df_data, 'left').drop_duplicates()\n",
    "        return(df_out)\n",
    "    \n",
    "    def add_ys(\n",
    "        self,\n",
    "        #df_envs = train_HPS,\n",
    "        df_data = meta,\n",
    "        add_cols = ['Date_Planted']\n",
    "        ):\n",
    "        self.train['y'] = self._mk_ys_df(\n",
    "            df_envs = self.train['set'],\n",
    "            df_data = df_data,\n",
    "            add_cols = add_cols)\n",
    "        \n",
    "        self.test['y'] = self._mk_ys_df(\n",
    "            df_envs = self.test['set'],\n",
    "            df_data = df_data,\n",
    "            add_cols = add_cols)\n",
    "        \n",
    "    #  add in x variables\n",
    "    def _mk_xs_df(\n",
    "        self,\n",
    "        df_envs, #= train_HPS,\n",
    "        df_data = wthr,\n",
    "        drop_cols = ['Year', 'Date', 'DOY']\n",
    "        ):    \n",
    "        df_data = df_data.drop(columns = [e for e in list(df_data) if e in drop_cols])\n",
    "        df_out = df_envs.merge(df_data, 'left').drop_duplicates()\n",
    "        df_out = df_out.drop(columns = [e for e in list(df_out) if e in drop_cols])\n",
    "        return(df_out)\n",
    "    \n",
    "    def add_xs(\n",
    "        self,\n",
    "        #df_envs = train_HPS,\n",
    "        df_data = wthr,\n",
    "        drop_cols = ['Year', 'Date', 'DOY']\n",
    "        ):\n",
    "        self.train['x'] = self._mk_xs_df(\n",
    "            df_envs = self.train['set'],\n",
    "            df_data = df_data,\n",
    "            drop_cols = drop_cols)\n",
    "\n",
    "        self.test['x'] = self._mk_xs_df(\n",
    "            df_envs = self.test['set'],\n",
    "            df_data = df_data,\n",
    "            drop_cols = drop_cols)\n",
    "    \n",
    "    ## Separate out missing responses ==========================================\n",
    "    def isolate_missing_y(self):\n",
    "        if self.train['y'] is not None:\n",
    "            mask = (self.train['y'].isnull().any(axis = 1))\n",
    "            self.train['yna'] = self.train['y'].loc[mask, :].copy()\n",
    "            self.train['y'] = self.train['y'].loc[~mask, :].copy()\n",
    "        if self.test['y'] is not None:\n",
    "            mask = (self.test['y'].isnull().any(axis = 1))\n",
    "            self.test['yna'] = self.test['y'].loc[mask, :].copy()\n",
    "            self.test['y'] = self.test['y'].loc[~mask, :].copy()\n",
    "            \n",
    "        self.isolate_missing_y_run = True  # This is just for a guard rail in mk_scale_dict\n",
    "            \n",
    "    def prep_idx_y(self):\n",
    "        if self.train['y'] is not None:\n",
    "            self.train['y'] = self.train['y'].reset_index().drop(columns = 'index')\n",
    "        if self.train['yna'] is not None:\n",
    "            self.train['yna'] = self.train['yna'].reset_index().drop(columns = 'index')\n",
    "            \n",
    "        if self.test['y'] is not None:\n",
    "            self.test['y'] = self.test['y'].reset_index().drop(columns = 'index') \n",
    "        if self.test['yna'] is not None:\n",
    "            self.test['yna'] = self.test['yna'].reset_index().drop(columns = 'index')\n",
    "            \n",
    "    ## Center &  Scaling dict ==================================================\n",
    "    # This looks odd (why not only have one method?) but is intentional. The idea here is that one\n",
    "    # might want to provide a saved center and scaling dictionary or include custom scaling for some \n",
    "    # columns. By including an update method making the default to return the dictionary instead of \n",
    "    # updating it silently it's easier to access and makes the step more visible. \n",
    "    def update_cs_dict(self, cs_dict):\n",
    "        self.cs_dict = cs_dict            \n",
    "    \n",
    "    def mk_scale_dict(self, \n",
    "                     scale_cols = ['Date_Planted'],\n",
    "                     return_cs_dict = True,\n",
    "                     store_cs_dict = False):\n",
    "        # scale df\n",
    "        if not self.isolate_missing_y_run: \n",
    "            print(\"Warning: if run before isolate_missing_y all observations will be used.\")\n",
    "            \n",
    "        temp = self.train['y'].merge(self.train['x'], how = 'left')\n",
    "        cs_dict = {}\n",
    "        for e in scale_cols:\n",
    "            cs_dict.update({e : {'mean': np.mean(temp[e]), \n",
    "                                 'std' : np.std(temp[e])}})\n",
    "            \n",
    "        if store_cs_dict:\n",
    "            if self.cs_dict is not None:\n",
    "                print('Overwriting Center and Scaling Dict.')\n",
    "            self.update_cs_dict(cs_dict)   \n",
    "            \n",
    "        if return_cs_dict:\n",
    "            return(cs_dict) \n",
    "\n",
    "    ## Scaling / reverse scaling by dict =======================================\n",
    "    def _scale_by_dict(self, df):\n",
    "        scale_cols = [e for e in list(self.cs_dict.keys()) if e in list(df)]\n",
    "        for e in scale_cols:\n",
    "            df[e] = (df[e] - self.cs_dict[e]['mean'])/self.cs_dict[e]['std']\n",
    "        return(df)\n",
    "    \n",
    "    def _unscale_by_dict(self, df):\n",
    "        scale_cols = [e for e in list(self.cs_dict.keys()) if e in list(df)]\n",
    "        for e in scale_cols:\n",
    "            df[e] = (df[e]*self.cs_dict[e]['std'])+self.cs_dict[e]['mean']\n",
    "        return(df)\n",
    "            \n",
    "    def apply_scaling(self):\n",
    "        if self.train[ 'y'] is not None:\n",
    "            self.train['y']   = self._scale_by_dict(self.train['y'])\n",
    "        if self.train[ 'yna'] is not None:\n",
    "            self.train['yna'] = self._scale_by_dict(self.train['yna'])\n",
    "        if self.train[ 'x'] is not None:\n",
    "            self.train['x']   = self._scale_by_dict(self.train['x'])            \n",
    "            \n",
    "        if self.test[ 'y'] is not None:\n",
    "            self.test['y']   = self._scale_by_dict(self.test['y']) \n",
    "        if self.test[ 'yna'] is not None:\n",
    "            self.test['yna'] = self._scale_by_dict(self.test['yna'])        \n",
    "        if self.test[ 'x'] is not None:\n",
    "            self.test['x']   = self._scale_by_dict(self.test['x']) \n",
    "        \n",
    "    def reverse_scaling(self):\n",
    "        if self.train[ 'y'] is not None:\n",
    "            self.train['y']   = self._unscale_by_dict(self.train['y'])\n",
    "        if self.train[ 'yna'] is not None:\n",
    "            self.train['yna'] = self._unscale_by_dict(self.train['yna'])\n",
    "        if self.train[ 'x'] is not None:\n",
    "            self.train['x']   = self._unscale_by_dict(self.train['x'])\n",
    "            \n",
    "        if self.test[ 'y'] is not None:\n",
    "            self.test['y']   = self._unscale_by_dict(self.test['y']) \n",
    "        if self.test[ 'yna'] is not None:\n",
    "            self.test['yna'] = self._unscale_by_dict(self.test['yna']) \n",
    "        if self.test[ 'x'] is not None:\n",
    "            self.test['x']   = self._unscale_by_dict(self.test['x'])\n",
    "      \n",
    "    \n",
    "    ## numpy arrays to easily be converted to tensors ==========================\n",
    "    def mk_arrays(\n",
    "        self,\n",
    "        split = 'train',\n",
    "        obs_per_Env = 1, \n",
    "        return_2d   = True,\n",
    "        missing_ys  = False):\n",
    "        # using the envs specified in the y data frame and the covariates in the x data frame, return numpy arrays \n",
    "        # with the xs and ys accessible by the same idx \n",
    "\n",
    "        def _reformat_xy(y_df, \n",
    "                         x_df, \n",
    "                         obs_per_Env, \n",
    "                         return_2d = False):\n",
    "            ys_tensor = np.array(y_df.loc[:, [e for e in y_df if e not in ['Env', 'Year']]])\n",
    "            # Don't seem to need to manually set second dim (to 1 from none) do this when data is drawn from df asabove\n",
    "            # ys_tensor = ys_tensor.reshape((ys_df.shape[0], 1))\n",
    "\n",
    "            col_names = [e for e in list(x_df) if e not in ['Env']]\n",
    "\n",
    "            num_obs = ys_tensor.shape[0] # observations\n",
    "            # obs_per_Env # user input, 1 in most cases, 314 for weather\n",
    "            col_per_obs = len(col_names)\n",
    "\n",
    "            xs_tensor = np.zeros(shape = (num_obs,\n",
    "                                          obs_per_Env,\n",
    "                                          col_per_obs)) \n",
    "\n",
    "            for i in y_df.index:\n",
    "                mask = (x_df.Env == y_df.loc[i, 'Env'])\n",
    "                xs_tensor[i, :, :] = np.array(x_df.loc[mask, col_names\n",
    "                                                       ].drop_duplicates() )\n",
    "                # Note! this will result in weather data being in order of \n",
    "                # N, Length (days), Channels. To match pytorch conventions \n",
    "                # I swap these below\n",
    "                \n",
    "            # for non weather data\n",
    "            if return_2d:\n",
    "                new_dim_0 = xs_tensor.shape[0]\n",
    "                new_dim_1 = xs_tensor.shape[2]\n",
    "                xs_tensor = xs_tensor.reshape(new_dim_0, new_dim_1)\n",
    "            else:\n",
    "                # swap axes so that weather is in order of \n",
    "                # N, Channels, Length\n",
    "                xs_tensor = xs_tensor.swapaxes(1,2)\n",
    "\n",
    "            return([ys_tensor, xs_tensor])\n",
    "\n",
    "        if split not in ['train', 'test']:\n",
    "            print('`split`must be \"train\" or \"test\"')\n",
    "        else:\n",
    "            if split == 'train':\n",
    "                if not missing_ys:\n",
    "                    out = _reformat_xy(\n",
    "                        y_df = self.train['y'], \n",
    "                        x_df = self.train['x'], \n",
    "                        obs_per_Env = obs_per_Env, \n",
    "                        return_2d = return_2d)\n",
    "                else:\n",
    "                    out = _reformat_xy(\n",
    "                        y_df = self.train['yna'], \n",
    "                        x_df = self.train['x'], \n",
    "                        obs_per_Env = obs_per_Env, \n",
    "                        return_2d = return_2d)\n",
    "\n",
    "            elif split == 'test':\n",
    "                if not missing_ys:\n",
    "                    out = _reformat_xy(\n",
    "                        y_df = self.test['y'], \n",
    "                        x_df = self.test['x'], \n",
    "                        obs_per_Env = obs_per_Env, \n",
    "                        return_2d = return_2d)\n",
    "                else:\n",
    "                    out = _reformat_xy(\n",
    "                        y_df = self.test['yna'], \n",
    "                        x_df = self.test['x'], \n",
    "                        obs_per_Env = obs_per_Env, \n",
    "                        return_2d = return_2d)\n",
    "            return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f467f996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dates to doy so they're easier to work with\n",
    "def df_date_to_datetime(df, cols):\n",
    "    temp = df.copy()\n",
    "    for col in cols:\n",
    "        temp[col] = [pd.Period(e, freq='D').day_of_year for e in list(temp[col])]\n",
    "    return(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c57dd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 3d data (wthr)\n",
    "if True == False:\n",
    "    demo = df_prep()\n",
    "    demo.get_train_test_Envs(\n",
    "        df = meta, \n",
    "        holdout_years = ['2020', '2014', '2016', '2022'],\n",
    "        test_year =     ['2020'] )\n",
    "    demo.add_ys(\n",
    "            df_data = df_date_to_datetime(df = meta, \n",
    "                                          cols = ['Date_Planted']),\n",
    "            add_cols = ['Date_Planted'])\n",
    "    demo.add_xs(\n",
    "            df_data = wthr,\n",
    "            drop_cols = ['Year', 'Date', 'DOY'])\n",
    "    demo.isolate_missing_y()\n",
    "    demo.prep_idx_y()\n",
    "    demo.mk_scale_dict(\n",
    "        scale_cols = ['Date_Planted']+[e for e in list(wthr) if e not in ['Env', 'Year', 'Date', 'DOY']],\n",
    "        return_cs_dict = False,\n",
    "        store_cs_dict = True)\n",
    "    demo.apply_scaling()\n",
    "    # demo.reverse_scaling()\n",
    "\n",
    "    train_y, train_x = demo.mk_arrays(\n",
    "        split = 'train',\n",
    "        obs_per_Env = 314, \n",
    "        return_2d   = False,\n",
    "        missing_ys  = False)\n",
    "\n",
    "    test_y, test_x = demo.mk_arrays(\n",
    "        split = 'test',\n",
    "        obs_per_Env = 314, \n",
    "        return_2d   = False,\n",
    "        missing_ys  = False)\n",
    "\n",
    "    print([e.shape for e in [train_y, train_x, test_y, test_x]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97131b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 2d data (soil)\n",
    "if True == False:\n",
    "    demo = df_prep()\n",
    "    demo.get_train_test_Envs(\n",
    "        df = meta, \n",
    "        holdout_years = ['2020', '2014', '2016', '2022'],\n",
    "        test_year =     ['2020'] )\n",
    "    demo.add_ys(\n",
    "            df_data = df_date_to_datetime(df = meta, \n",
    "                                          cols = ['Date_Planted']),\n",
    "            add_cols = ['Date_Planted'])\n",
    "    demo.add_xs(\n",
    "            df_data = soil, \n",
    "            drop_cols = ['Year', 'Date', 'DOY'])\n",
    "    demo.isolate_missing_y()\n",
    "    demo.prep_idx_y()\n",
    "    demo.mk_scale_dict(\n",
    "        scale_cols = ['Date_Planted']+[e for e in list(soil) if e not in ['Env', 'Year', 'Date', 'DOY']],\n",
    "        return_cs_dict = False,\n",
    "        store_cs_dict = True)\n",
    "    demo.apply_scaling()\n",
    "    # demo.reverse_scaling()\n",
    "\n",
    "    train_y, train_x = demo.mk_arrays(\n",
    "        split = 'train',\n",
    "        obs_per_Env = 1, \n",
    "        return_2d   = True,\n",
    "        missing_ys  = False)\n",
    "\n",
    "    test_y, test_x = demo.mk_arrays(\n",
    "        split = 'test',\n",
    "        obs_per_Env = 1, \n",
    "        return_2d   = True,\n",
    "        missing_ys  = False)\n",
    "\n",
    "\n",
    "    print([e.shape for e in [train_y, train_x, test_y, test_x]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c08a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the input data\n",
    "class CustomDataset_wthr(Dataset):\n",
    "    def __init__(self, y, x, \n",
    "                 transform=None, # can pass in ToTensor()\n",
    "                 target_transform=None):\n",
    "        self.y = y\n",
    "        self.x = x\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x_idx = self.x[idx]\n",
    "        y_idx = self.y[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            x_idx = self.transform(x_idx)\n",
    "        if self.target_transform:\n",
    "            y_idx = self.target_transform(y_idx)\n",
    "        return x_idx, y_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803e559e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, silent = True):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            if not silent:\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, silent = True):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "#     correct /= size\n",
    "    if not silent:\n",
    "        print(f\"Test Error: Avg loss: {test_loss:>8f}\")\n",
    "    return(test_loss) #new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c01e4f1",
   "metadata": {},
   "source": [
    "## Shared Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b3de63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Demo 3d data (wthr)\n",
    "\n",
    "# demo = df_prep()\n",
    "# demo.get_train_test_Envs(df = meta, \n",
    "#                          holdout_years = ['2020', '2014', '2016', '2022'], \n",
    "#                          test_year =     ['2020'] )\n",
    "# demo.add_ys(df_data = df_date_to_datetime(df = meta, cols = ['Date_Planted']),\n",
    "#             add_cols = ['Date_Planted'])\n",
    "# demo.add_xs(df_data = wthr, \n",
    "#             drop_cols = ['Year', 'Date', 'DOY'])\n",
    "# demo.isolate_missing_y()\n",
    "# demo.prep_idx_y()\n",
    "# demo.mk_scale_dict(\n",
    "#     scale_cols = ['Date_Planted']+[e for e in list(wthr) if e not in ['Env', 'Year', 'Date', 'DOY']],\n",
    "#     return_cs_dict = False,\n",
    "#     store_cs_dict = True)\n",
    "# demo.apply_scaling()\n",
    "\n",
    "# train_y, train_x = demo.mk_arrays(\n",
    "#     split = 'train',\n",
    "#     obs_per_Env = 314, \n",
    "#     return_2d   = False,\n",
    "#     missing_ys  = False)\n",
    "\n",
    "# test_y, test_x = demo.mk_arrays(\n",
    "#     split = 'test',\n",
    "#     obs_per_Env = 314, \n",
    "#     return_2d   = False,\n",
    "#     missing_ys  = False)\n",
    "\n",
    "# [e.shape for e in [train_y, train_x, test_y, test_x]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add818e6",
   "metadata": {},
   "source": [
    "## PyTorch Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c60d774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_y_tensor = torch.from_numpy(train_y).to(device).float()\n",
    "# train_x_tensor = torch.from_numpy(train_x).to(device).float()\n",
    "\n",
    "# test_y_tensor = torch.from_numpy(test_y).to(device).float()\n",
    "# test_x_tensor = torch.from_numpy(test_x).to(device).float()\n",
    "\n",
    "# training_dataloader = DataLoader(CustomDataset_wthr(y = train_y_tensor, x = train_x_tensor), batch_size = 64, shuffle = True)\n",
    "# testing_dataloader = DataLoader(CustomDataset_wthr(y = test_y_tensor, x = test_x_tensor), batch_size = 64, shuffle = True)\n",
    "\n",
    "\n",
    "# class NeuralNetwork(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(NeuralNetwork, self).__init__()    \n",
    "#         in_size = 314 * 16\n",
    "#         n1 = 16 \n",
    "#         n2 = 16 \n",
    "    \n",
    "#         self.linear_relu_stack = nn.Sequential(\n",
    "#         nn.Flatten(),            \n",
    "#         nn.Linear(in_size, n1),\n",
    "#         nn.ReLU(),            \n",
    "#         nn.Linear(n1, n2),\n",
    "#         nn.ReLU(),            \n",
    "#         nn.Linear(n2, 1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x = self.flatten(x)\n",
    "#         logits = self.linear_relu_stack(x)\n",
    "#         return logits\n",
    "    \n",
    "    \n",
    "# model = NeuralNetwork().to(device)\n",
    "# # print(model)\n",
    "# # print(model(torch.rand(1,  device=device))) # shows that it works\n",
    "# # model(next(iter(training_dataloader))[0] ) # try prediction on one batch\n",
    "\n",
    "\n",
    "\n",
    "# # learning_rate = 1e-3\n",
    "# # batch_size = 64\n",
    "# # epochs = 500\n",
    "# #\n",
    "# # # Initialize the loss function\n",
    "# # loss_fn = nn.MSELoss()\n",
    "# # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# #\n",
    "# # loss_df = pd.DataFrame([i for i in range(epochs)], columns = ['Epoch'])\n",
    "# # loss_df['MSE'] = np.nan\n",
    "# #\n",
    "# # for t in tqdm.tqdm(range(epochs)):\n",
    "# #     # print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "# #     train_loop(training_dataloader, model, loss_fn, optimizer)\n",
    "# #    \n",
    "# #     loss_df.loc[loss_df.index == t, 'MSE'\n",
    "# #                ] = test_loop(testing_dataloader, model, loss_fn)\n",
    "# #\n",
    "# # print(\"Done!\")\n",
    "\n",
    "\n",
    "\n",
    "# def train_nn(\n",
    "#     training_dataloader,\n",
    "#     testing_dataloader,\n",
    "#     model,\n",
    "#     learning_rate = 1e-3,\n",
    "#     batch_size = 64,\n",
    "#     epochs = 500\n",
    "# ):\n",
    "#     # Initialize the loss function\n",
    "#     loss_fn = nn.MSELoss()\n",
    "#     optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#     loss_df = pd.DataFrame([i for i in range(epochs)], columns = ['Epoch'])\n",
    "#     loss_df['MSE'] = np.nan\n",
    "\n",
    "#     for t in tqdm.tqdm(range(epochs)):\n",
    "#         # print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "#         train_loop(training_dataloader, model, loss_fn, optimizer)\n",
    "\n",
    "#         loss_df.loc[loss_df.index == t, 'MSE'\n",
    "#                    ] = test_loop(testing_dataloader, model, loss_fn)\n",
    "\n",
    "#     # print(\"Done!\")\n",
    "#     return([model, loss_df])\n",
    "\n",
    "\n",
    "# model, loss_df = train_nn(\n",
    "#     training_dataloader,\n",
    "#     testing_dataloader,\n",
    "#     model,\n",
    "#     learning_rate = 1e-3,\n",
    "#     batch_size = 64,\n",
    "#     epochs = 500\n",
    "# )\n",
    "\n",
    "\n",
    "# yhats = model(test_x_tensor)\n",
    "# yhats = yhats.cpu().detach().numpy()\n",
    "\n",
    "# yobs = test_y_tensor\n",
    "# yobs = yobs.cpu().detach().numpy()\n",
    "\n",
    "# plt_df = pd.concat([\n",
    "#     pd.DataFrame(yhats, columns = ['yHat']),\n",
    "#     pd.DataFrame(yobs, columns = ['yObs'])], \n",
    "#     axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb927ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# px.line(loss_df, x = 'Epoch', y = 'MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbae659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# px.scatter(plt_df, x = 'yObs', y = 'yHat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456a4b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_squared_error(plt_df['yObs'], plt_df['yHat'], squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a467b276",
   "metadata": {},
   "source": [
    "### Simple dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f85176",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_this_year = '2021'\n",
    "# Setup ----------------------------------------------------------------------\n",
    "trial_name = 'DNN_hps_test'\n",
    "\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 500\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "    \n",
    "        in_size = 314 * 16\n",
    "        n1 = 2**10\n",
    "        n2 = 2**8\n",
    "        n3 = 2**6\n",
    "        n4 = 2**4\n",
    "        n5 = 2 \n",
    "    \n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "        nn.Flatten(),            \n",
    "        nn.Linear(in_size, n1),\n",
    "        nn.ReLU(),            \n",
    "        nn.Linear(n1, n2),\n",
    "        nn.ReLU(),            \n",
    "        nn.Linear(n2, n3),\n",
    "        nn.ReLU(),            \n",
    "        nn.Linear(n3, n4),\n",
    "        nn.ReLU(),            \n",
    "        nn.Linear(n4, n5),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(n5, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "def run_dnn_trial(\n",
    "    trial_name = 'DNN_hps_test',\n",
    "    learning_rate = 1e-3,\n",
    "    batch_size = 64,\n",
    "    epochs = 500):\n",
    "    reset_trial_name = trial_name\n",
    "    for test_this_year in ['2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021']:\n",
    "        trial_name = reset_trial_name\n",
    "        trial_name = trial_name+test_this_year\n",
    "        print(test_this_year)\n",
    "        # Data Prep. -----------------------------------------------------------------\n",
    "        data_obj = df_prep()\n",
    "        data_obj.get_train_test_Envs(df = meta, \n",
    "                                 holdout_years = [],\n",
    "                                 test_year =     [test_this_year] )\n",
    "        data_obj.add_ys(df_data = df_date_to_datetime(df = meta, cols = ['Date_Planted']),\n",
    "                    add_cols = ['Date_Planted'])\n",
    "        data_obj.add_xs(df_data = wthr, \n",
    "                    drop_cols = ['Year', 'Date', 'DOY'])\n",
    "        data_obj.isolate_missing_y()\n",
    "        data_obj.prep_idx_y()\n",
    "        data_obj.mk_scale_dict(\n",
    "            scale_cols = ['Date_Planted']+[e for e in list(wthr) if e not in ['Env', 'Year', 'Date', 'DOY']],\n",
    "            return_cs_dict = False,\n",
    "            store_cs_dict = True)\n",
    "        data_obj.apply_scaling()\n",
    "\n",
    "        train_y, train_x = data_obj.mk_arrays(\n",
    "            split = 'train',\n",
    "            obs_per_Env = 314, \n",
    "            return_2d   = False,\n",
    "            missing_ys  = False)\n",
    "\n",
    "        test_y, test_x = data_obj.mk_arrays(\n",
    "            split = 'test',\n",
    "            obs_per_Env = 314, \n",
    "            return_2d   = False,\n",
    "            missing_ys  = False)\n",
    "\n",
    "\n",
    "        train_y_tensor = torch.from_numpy(train_y).to(device).float()\n",
    "        train_x_tensor = torch.from_numpy(train_x).to(device).float()\n",
    "\n",
    "        test_y_tensor = torch.from_numpy(test_y).to(device).float()\n",
    "        test_x_tensor = torch.from_numpy(test_x).to(device).float()\n",
    "\n",
    "\n",
    "        training_dataloader = DataLoader(CustomDataset_wthr(y = train_y_tensor, x = train_x_tensor), batch_size = 64, shuffle = True)\n",
    "        testing_dataloader = DataLoader(CustomDataset_wthr(y = test_y_tensor, x = test_x_tensor), batch_size = 64, shuffle = True)\n",
    "\n",
    "        # Fit Mod --------------------------------------------------------------------   \n",
    "        cache_save_name = cache_path+trial_name+'_mod.pth'\n",
    "        if os.path.exists(cache_save_name):\n",
    "            model = torch.load(cache_save_name)\n",
    "            model = NeuralNetwork().to(device)\n",
    "\n",
    "        else:\n",
    "            model = NeuralNetwork().to(device)\n",
    "            def train_nn(\n",
    "                training_dataloader,\n",
    "                testing_dataloader,\n",
    "                model,\n",
    "                learning_rate = 1e-3,\n",
    "                batch_size = 64,\n",
    "                epochs = 500\n",
    "            ):\n",
    "                # Initialize the loss function\n",
    "                loss_fn = nn.MSELoss()\n",
    "                optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "                loss_df = pd.DataFrame([i for i in range(epochs)], columns = ['Epoch'])\n",
    "                loss_df['MSE'] = np.nan\n",
    "\n",
    "                for t in tqdm.tqdm(range(epochs)):\n",
    "                    # print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "                    train_loop(training_dataloader, model, loss_fn, optimizer)\n",
    "\n",
    "                    loss_df.loc[loss_df.index == t, 'MSE'\n",
    "                               ] = test_loop(testing_dataloader, model, loss_fn)\n",
    "                return([model, loss_df])\n",
    "\n",
    "\n",
    "            model, loss_df = train_nn(\n",
    "                training_dataloader,\n",
    "                testing_dataloader,\n",
    "                model,\n",
    "                learning_rate = learning_rate,\n",
    "                batch_size = batch_size,\n",
    "                epochs = epochs\n",
    "            )\n",
    "            # Save\n",
    "            torch.save(model, cache_save_name)\n",
    "            loss_df.to_csv(cache_path+trial_name+'_loss.csv', index = False)\n",
    "\n",
    "\n",
    "        # Eval. Best HPS -------------------------------------------------------------    \n",
    "        for i in range(4):\n",
    "            # u denotes that the true value is unknown\n",
    "            temp_label= ['train_y', 'test_y', 'train_u', 'test_u'][i]\n",
    "            if os.path.exists(cache_path+trial_name+temp_label+\".csv\"):\n",
    "                pass\n",
    "            else:\n",
    "                temp_data = [data_obj.train['y'], data_obj.test['y'], data_obj.train['yna'], data_obj.test['yna']][i]\n",
    "\n",
    "                if temp_data.shape[0] != 0:\n",
    "                    # calculation step below =============================================\n",
    "                    temp_y, temp_x = data_obj.mk_arrays(                                 #\n",
    "                        split = temp_label.split('_')[0],                                #\n",
    "                        obs_per_Env = 314,                                               #\n",
    "                        return_2d   = False,                                             #\n",
    "                        missing_ys  = temp_label.split('_')[1] == 'u' )                  #\n",
    "                                                                                         #\n",
    "                    temp_yHat = model(torch.from_numpy(temp_x).to(device).float())       #\n",
    "                    temp_yHat = temp_yHat.cpu().detach().numpy()                         #\n",
    "                    temp_yHat.reshape(temp_yHat.shape[0], ) # collapse to 1d #\n",
    "                    # calculation step above =============================================\n",
    "                    temp_data['yHat'] = [e[0] for e in list(temp_yHat)]\n",
    "                    temp_data.to_csv(cache_path+trial_name+'_'+temp_label+\".csv\", index = False)\n",
    "\n",
    "                if temp_label.split('_')[1] != 'u':\n",
    "                    pd.DataFrame({'MSE':[mean_squared_error(temp_y, temp_yHat)]}).to_csv(cache_path+trial_name+'_'+temp_label+\"_mse.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef460583",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    run_dnn_trial(\n",
    "        trial_name = 'DNN_hps_test',\n",
    "        learning_rate = 1e-3,\n",
    "        batch_size = 64,\n",
    "        epochs = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56cce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "    \n",
    "        in_size = 314 * 16\n",
    "        n1 = 2**10\n",
    "        n4 = 2**4\n",
    "        n5 = 2 \n",
    "    \n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "        nn.Flatten(),            \n",
    "        nn.Linear(in_size, n1),\n",
    "        nn.Linear(n1, n4),\n",
    "        nn.ReLU(),            \n",
    "        nn.Linear(n4, n5),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(n5, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "if False:\n",
    "    run_dnn_trial(\n",
    "        trial_name = 'DNNsmall_hps_test',\n",
    "        learning_rate = 1e-3,\n",
    "        batch_size = 64,\n",
    "        epochs = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa03c909",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()  \n",
    "        self.seq_model = nn.Sequential(\n",
    "            nn.Conv1d(16, 32, kernel_size = 3, stride=2), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 16, kernel_size = 3, stride=2), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 8, kernel_size = 3, stride=2),       \n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(8, 4, kernel_size = 3, stride=2),       \n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(4, 2, kernel_size = 3, stride=2),       \n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.flatten(x)\n",
    "        logits = self.seq_model(x)\n",
    "        return logits\n",
    "    \n",
    "if False:       \n",
    "    run_dnn_trial(\n",
    "        trial_name = 'CNNv1_hps_test',\n",
    "        learning_rate = 1e-3,\n",
    "        batch_size = 64,\n",
    "        epochs = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e9691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to help with checking dims\n",
    "\n",
    "# test_x_ten = torch.from_numpy(test_x).float()\n",
    "# test_x_ten.shape\n",
    "\n",
    "# #          Out shape\n",
    "# #      channels    |\n",
    "# #             |    |\n",
    "# m = nn.Sequential(\n",
    "#     nn.Conv1d(16, 32, kernel_size = 3, stride=2), \n",
    "#     nn.ReLU(),\n",
    "#     nn.Conv1d(32, 16, kernel_size = 3, stride=2), \n",
    "#     nn.ReLU(),\n",
    "#     nn.Conv1d(16, 8, kernel_size = 3, stride=2),       \n",
    "#     nn.ReLU(),\n",
    "#     nn.Conv1d(8, 4, kernel_size = 3, stride=2),       \n",
    "#     nn.ReLU(),\n",
    "#     nn.Conv1d(4, 2, kernel_size = 3, stride=2),       \n",
    "#     nn.ReLU(),\n",
    "#     nn.Flatten(),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(16, 1)\n",
    "# )\n",
    "\n",
    "# m(test_x_ten).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53ab9f5",
   "metadata": {},
   "source": [
    "### Reflect on DNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f21546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_files = [e for e in os.listdir(cache_path) if re.match('.+_loss.csv$', e)]\n",
    "\n",
    "loss_dfs = [pd.read_csv(cache_path+e) for e in loss_files]\n",
    "for i in range(len(loss_files)):\n",
    "    loss_dfs[i]['File'] = loss_files[i]\n",
    "    \n",
    "loss_df = pd.concat(loss_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558fa7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df[['Model', 'Stage', 'CV', 'Record']] = loss_df['File'].str.split('_', expand = True)\n",
    "loss_df['CV'] = loss_df['CV'].str.strip('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4aa8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(loss_df, x = 'Epoch', y = 'MSE', color = 'CV', facet_col=\"Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0666167e",
   "metadata": {},
   "source": [
    "## Sklearn Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3610180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to panel data\n",
    "def wthr_rank_3to2(x_3d):\n",
    "    n_obs, n_days, n_metrics = x_3d.shape\n",
    "    return(x_3d.reshape(n_obs, (n_days*n_metrics)))\n",
    "\n",
    "def y_rank_2to1(y_2d):\n",
    "    n_obs = y_2d.shape[0]\n",
    "    return(y_2d.reshape(n_obs, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c5b724",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f508ae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x_2d = wthr_rank_3to2(x_3d = train_x)\n",
    "# train_y_1d = y_rank_2to1(y_2d = train_y)\n",
    "# regr = RandomForestRegressor(max_depth= 16, \n",
    "#                              random_state=0,\n",
    "#                              n_estimators = 20)\n",
    "# rf = regr.fit(train_x_2d, train_y_1d)\n",
    "# mean_squared_error(train_y_1d, rf.predict(train_x_2d), squared=False)\n",
    "# px.bar(pd.DataFrame(dict(cols=trn_xs.columns, imp=rf.feature_importances_)), x = 'cols', y = 'imp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3980f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_this_year = '2021'\n",
    "\n",
    "# Setup ----------------------------------------------------------------------\n",
    "trial_name = 'rf_hps_test'\n",
    "\n",
    "n_trials= 200 \n",
    "n_jobs = 20\n",
    "\n",
    "def objective(trial):\n",
    "    rf_max_depth = trial.suggest_int('rf_max_depth', 2, 200, log=True)\n",
    "    rf_n_estimators = trial.suggest_int('rf_n_estimators', 2, 200, log=True)\n",
    "    rf_min_samples_split = trial.suggest_float('rf_min_samples_split', 0.01, 0.99, log=True)\n",
    "    \n",
    "    regr = RandomForestRegressor(\n",
    "        max_depth = rf_max_depth, \n",
    "        n_estimators = rf_n_estimators,\n",
    "        min_samples_split = rf_min_samples_split\n",
    "        )\n",
    "    \n",
    "    rf = regr.fit(train_x_2d, train_y_1d)\n",
    "    return (mean_squared_error(train_y_1d, rf.predict(train_x_2d), squared=False))\n",
    "\n",
    "\n",
    "reset_trial_name = trial_name\n",
    "\n",
    "if False:\n",
    "    for test_this_year in ['2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021']:\n",
    "        trial_name = reset_trial_name\n",
    "        trial_name = trial_name+test_this_year\n",
    "        print(test_this_year)\n",
    "        # Data Prep. -----------------------------------------------------------------\n",
    "        data_obj = df_prep()\n",
    "        data_obj.get_train_test_Envs(df = meta, \n",
    "                                 holdout_years = [],\n",
    "                                 test_year =     [test_this_year] )\n",
    "        data_obj.add_ys(df_data = df_date_to_datetime(df = meta, cols = ['Date_Planted']),\n",
    "                    add_cols = ['Date_Planted'])\n",
    "        data_obj.add_xs(df_data = wthr, \n",
    "                    drop_cols = ['Year', 'Date', 'DOY'])\n",
    "        data_obj.isolate_missing_y()\n",
    "        data_obj.prep_idx_y()\n",
    "        data_obj.mk_scale_dict(\n",
    "            scale_cols = ['Date_Planted']+[e for e in list(wthr) if e not in ['Env', 'Year', 'Date', 'DOY']],\n",
    "            return_cs_dict = False,\n",
    "            store_cs_dict = True)\n",
    "        data_obj.apply_scaling()\n",
    "\n",
    "        train_y, train_x = data_obj.mk_arrays(\n",
    "            split = 'train',\n",
    "            obs_per_Env = 314, \n",
    "            return_2d   = False,\n",
    "            missing_ys  = False)\n",
    "\n",
    "        test_y, test_x = data_obj.mk_arrays(\n",
    "            split = 'test',\n",
    "            obs_per_Env = 314, \n",
    "            return_2d   = False,\n",
    "            missing_ys  = False)\n",
    "\n",
    "\n",
    "        train_x_2d = wthr_rank_3to2(x_3d = train_x)\n",
    "        train_y_1d = y_rank_2to1(y_2d = train_y)\n",
    "\n",
    "\n",
    "        # HPS Study ------------------------------------------------------------------\n",
    "        cache_save_name = cache_path+trial_name+'_hps.pkl'\n",
    "        if os.path.exists(cache_save_name):\n",
    "            study = pkl.load(open(cache_save_name, 'rb'))  \n",
    "        else:\n",
    "            study = optuna.create_study(direction=\"minimize\")\n",
    "            study.optimize(objective, n_trials= n_trials, n_jobs = n_jobs)\n",
    "            # save    \n",
    "            pkl.dump(study, open(cache_save_name, 'wb'))    \n",
    "\n",
    "\n",
    "        # Fit Best HPS ---------------------------------------------------------------   \n",
    "        cache_save_name = cache_path+trial_name+'_mod.pkl'\n",
    "        if os.path.exists(cache_save_name):\n",
    "            rf = pkl.load(open(cache_save_name, 'rb'))  \n",
    "        else:\n",
    "            regr = RandomForestRegressor(\n",
    "                    max_depth = study.best_trial.params['rf_max_depth'], \n",
    "                    n_estimators = study.best_trial.params['rf_n_estimators'],\n",
    "                    min_samples_split = study.best_trial.params['rf_min_samples_split']\n",
    "                    )\n",
    "            rf = regr.fit(train_x_2d, train_y_1d)\n",
    "            # save    \n",
    "            pkl.dump(rf, open(cache_save_name, 'wb'))   \n",
    "\n",
    "\n",
    "            # Eval. Best HPS -------------------------------------------------------------    \n",
    "            for i in range(4):\n",
    "                # u denotes that the true value is unknown\n",
    "                temp_label= ['train_y', 'test_y', 'train_u', 'test_u'][i]\n",
    "                if os.path.exists(cache_path+trial_name+temp_label+\".csv\"):\n",
    "                    pass\n",
    "                else:\n",
    "                    temp_data = [data_obj.train['y'], data_obj.test['y'], data_obj.train['yna'], data_obj.test['yna']][i]\n",
    "\n",
    "                    if temp_data.shape[0] != 0:\n",
    "                        # calculation step below =============================================\n",
    "                        temp_y, temp_x = data_obj.mk_arrays(                                 #\n",
    "                            split = temp_label.split('_')[0],                                #\n",
    "                            obs_per_Env = 314,                                               #\n",
    "                            return_2d   = False,                                             #\n",
    "                            missing_ys  = temp_label.split('_')[1] == 'u' )                  #\n",
    "                                                                                             #\n",
    "                        temp_y = y_rank_2to1(y_2d = temp_y)                                  #\n",
    "                        temp_yHat = rf.predict(wthr_rank_3to2(x_3d = temp_x))                #\n",
    "                        # calculation step above =============================================\n",
    "                        temp_data['yHat'] = list(temp_yHat)\n",
    "                        temp_data.to_csv(cache_path+trial_name+'_'+temp_label+\".csv\", index = False)\n",
    "\n",
    "                        if temp_label.split('_')[1] != 'u':\n",
    "                            pd.DataFrame({'MSE':[mean_squared_error(temp_y, temp_yHat)]}).to_csv(cache_path+trial_name+'_'+temp_label+\"_mse.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3395f8f4",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cabe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec6b212",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_this_year = '2021'\n",
    "\n",
    "# Setup ----------------------------------------------------------------------\n",
    "trial_name = 'xgb_hps_test'\n",
    "\n",
    "n_trials= 200 # FIXME\n",
    "n_jobs = 20\n",
    "\n",
    "def objective(trial):\n",
    "    xgb_max_depth = trial.suggest_int('xgb_max_depth', 2, 200, log=True)\n",
    "    xgb_n_estimators = trial.suggest_int('xgb_n_estimators', 2, 200, log=True)\n",
    "    xgb_learning_rate = trial.suggest_float('xgb_learning_rate', 0.0001, 0.3, log=True)\n",
    "    \n",
    "    regr = XGBRegressor(\n",
    "        max_depth = xgb_max_depth, \n",
    "        n_estimators = xgb_n_estimators,\n",
    "        learning_rate = xgb_learning_rate,\n",
    "        objective='reg:squarederror'\n",
    "        )\n",
    "    \n",
    "    xgb = regr.fit(train_x_2d, train_y_1d)\n",
    "    return (mean_squared_error(train_y_1d, xgb.predict(train_x_2d), squared=False))\n",
    "\n",
    "\n",
    "reset_trial_name = trial_name\n",
    "\n",
    "if False:\n",
    "    for test_this_year in ['2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021']:\n",
    "        trial_name = reset_trial_name\n",
    "        trial_name = trial_name+test_this_year\n",
    "        print(test_this_year)\n",
    "        # Data Prep. -----------------------------------------------------------------\n",
    "        data_obj = df_prep()\n",
    "        data_obj.get_train_test_Envs(df = meta, \n",
    "                                 holdout_years = [],\n",
    "                                 test_year =     [test_this_year] )\n",
    "        data_obj.add_ys(df_data = df_date_to_datetime(df = meta, cols = ['Date_Planted']),\n",
    "                    add_cols = ['Date_Planted'])\n",
    "        data_obj.add_xs(df_data = wthr, \n",
    "                    drop_cols = ['Year', 'Date', 'DOY'])\n",
    "        data_obj.isolate_missing_y()\n",
    "        data_obj.prep_idx_y()\n",
    "        data_obj.mk_scale_dict(\n",
    "            scale_cols = ['Date_Planted']+[e for e in list(wthr) if e not in ['Env', 'Year', 'Date', 'DOY']],\n",
    "            return_cs_dict = False,\n",
    "            store_cs_dict = True)\n",
    "        data_obj.apply_scaling()\n",
    "\n",
    "        train_y, train_x = data_obj.mk_arrays(\n",
    "            split = 'train',\n",
    "            obs_per_Env = 314, \n",
    "            return_2d   = False,\n",
    "            missing_ys  = False)\n",
    "\n",
    "        test_y, test_x = data_obj.mk_arrays(\n",
    "            split = 'test',\n",
    "            obs_per_Env = 314, \n",
    "            return_2d   = False,\n",
    "            missing_ys  = False)\n",
    "\n",
    "\n",
    "        train_x_2d = wthr_rank_3to2(x_3d = train_x)\n",
    "        train_y_1d = y_rank_2to1(y_2d = train_y)\n",
    "\n",
    "\n",
    "        # HPS Study ------------------------------------------------------------------\n",
    "        cache_save_name = cache_path+trial_name+'_hps.pkl'\n",
    "        if os.path.exists(cache_save_name):\n",
    "            study = pkl.load(open(cache_save_name, 'rb'))  \n",
    "        else:\n",
    "            study = optuna.create_study(direction=\"minimize\")\n",
    "            study.optimize(objective, n_trials= n_trials, n_jobs = n_jobs)\n",
    "            # save    \n",
    "            pkl.dump(study, open(cache_save_name, 'wb'))    \n",
    "\n",
    "\n",
    "        # Fit Best HPS ---------------------------------------------------------------   \n",
    "        cache_save_name = cache_path+trial_name+'_mod.pkl'\n",
    "        if os.path.exists(cache_save_name):\n",
    "            xgb = pkl.load(open(cache_save_name, 'rb'))  \n",
    "        else:\n",
    "            regr = XGBRegressor(\n",
    "                    max_depth = study.best_trial.params['xgb_max_depth'], \n",
    "                    n_estimators = study.best_trial.params['xgb_n_estimators'],\n",
    "                    learning_rate = study.best_trial.params['xgb_learning_rate'],\n",
    "                    objective='reg:squarederror'\n",
    "                    )\n",
    "\n",
    "            xgb = regr.fit(train_x_2d, train_y_1d)\n",
    "            # save    \n",
    "            pkl.dump(xgb, open(cache_save_name, 'wb'))   \n",
    "\n",
    "            # Eval. Best HPS -------------------------------------------------------------    \n",
    "            for i in range(4):\n",
    "                # u denotes that the true value is unknown\n",
    "                temp_label= ['train_y', 'test_y', 'train_u', 'test_u'][i]\n",
    "                if os.path.exists(cache_path+trial_name+temp_label+\".csv\"):\n",
    "                    pass\n",
    "                else:\n",
    "                    temp_data = [data_obj.train['y'], data_obj.test['y'], data_obj.train['yna'], data_obj.test['yna']][i]\n",
    "\n",
    "                    if temp_data.shape[0] != 0:\n",
    "                        # calculation step below =============================================\n",
    "                        temp_y, temp_x = data_obj.mk_arrays(                                 #\n",
    "                            split = temp_label.split('_')[0],                                #\n",
    "                            obs_per_Env = 314,                                               #\n",
    "                            return_2d   = False,                                             #\n",
    "                            missing_ys  = temp_label.split('_')[1] == 'u' )                  #\n",
    "                                                                                             #\n",
    "                        temp_y = y_rank_2to1(y_2d = temp_y)                                  #\n",
    "                        temp_yHat = xgb.predict(wthr_rank_3to2(x_3d = temp_x))               #\n",
    "                        # calculation step above =============================================\n",
    "                        temp_data['yHat'] = list(temp_yHat)\n",
    "                        temp_data.to_csv(cache_path+trial_name+'_'+temp_label+\".csv\", index = False)\n",
    "\n",
    "                        if temp_label.split('_')[1] != 'u':\n",
    "                            pd.DataFrame({'MSE':[mean_squared_error(temp_y, temp_yHat)]}).to_csv(cache_path+trial_name+'_'+temp_label+\"_mse.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d747f8c",
   "metadata": {},
   "source": [
    "## Aggregate Estimates\n",
    "Test out inverse variance weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ae336c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all errors\n",
    "mse_files = [e for e in os.listdir(cache_path) if re.match('.+_mse.csv$', e)]\n",
    "mse_files = [e for e in mse_files if re.match('.+hps.+', e)]\n",
    "\n",
    "mod_mses = [pd.read_csv(cache_path+e) for e in mse_files]\n",
    "for i in range(len(mse_files)):\n",
    "    mod_mses[i]['File'] = mse_files[i]\n",
    "    \n",
    "mod_mse = pd.concat(mod_mses)\n",
    "\n",
    "mod_mse[['Model', 'Stage', 'CV', 'Set', 'Discard1', 'Discard2']] = mod_mse['File'].str.split('_', expand = True)\n",
    "mod_mse = mod_mse.drop(columns = ['Discard1', 'Discard2'])\n",
    "mod_mse['CV'] = mod_mse['CV'].str.strip('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6957741",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_mse.loc[mod_mse.Model == 'rf', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4eb0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(mod_mse, x = \"Model\", y = 'MSE', color = 'CV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f65038",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = [e for e in os.listdir(cache_path) if re.match('.+_test_y.csv$', e)]\n",
    "test_files = [e for e in test_files if re.match('.+hps.+', e)]\n",
    "\n",
    "test_preds = [pd.read_csv(cache_path+e) for e in test_files]\n",
    "for i in range(len(test_files)):\n",
    "    test_preds[i]['File'] = test_files[i]\n",
    "    \n",
    "test_pred = pd.concat(test_preds)\n",
    "\n",
    "test_pred[['Model', 'Stage', 'CV', 'Set', 'Discard1']] = test_pred['File'].str.split('_', expand = True)\n",
    "test_pred = test_pred.drop(columns = ['Discard1'])\n",
    "test_pred['CV'] = test_pred['CV'].str.strip('test')\n",
    "\n",
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0ad692",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_wide = test_pred.pivot(columns='Model', values='yHat', index = ['Env', 'Year', 'CV', 'Date_Planted'])\n",
    "test_pred_wide = test_pred_wide.reset_index()\n",
    "test_pred_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24b95b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter_matrix(test_pred_wide, dimensions=['Date_Planted', 'CNNv1', 'DNN', 'DNNsmall', 'rf', 'xgb'], \n",
    "                 color = 'CV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb617146",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_list = [e for e in list(test_pred_wide) if e not in ['Env', 'Year', 'CV', 'Date_Planted']]\n",
    "var_list = [(test_pred_wide['Date_Planted'] - test_pred_wide[e]).var() for e in mod_list]\n",
    "wht_list = [e/np.sum(var_list) for e in var_list]\n",
    "\n",
    "\n",
    "# Aggregate estimates using averaging and inverse variance weighting\n",
    "for i in range(len(mod_list)):\n",
    "    if i == 0:\n",
    "        yHat_ave_accumulator = test_pred_wide[mod_list[i]]*(1/len(mod_list))\n",
    "    else:\n",
    "        yHat_ave_accumulator += test_pred_wide[mod_list[i]]*(1/len(mod_list))\n",
    "\n",
    "for i in range(len(mod_list)):\n",
    "    if i == 0:\n",
    "        yHat_invVar_accumulator = test_pred_wide[mod_list[i]]*wht_list[i]\n",
    "    else:\n",
    "        yHat_invVar_accumulator += test_pred_wide[mod_list[i]]*wht_list[i]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45de8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_wide['mean'] = yHat_ave_accumulator\n",
    "test_pred_wide['iVar'] = yHat_invVar_accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edb4760",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_long = test_pred_wide.melt(id_vars=['Env', 'Year', 'CV', 'Date_Planted'])\n",
    "test_pred_long['ErrorSq'] = (test_pred_long['Date_Planted'] - test_pred_long['value'])**2\n",
    "test_pred_long = test_pred_long.groupby(['CV', 'Model']).agg(MSE = ('ErrorSq', np.mean)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec9481c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed59e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(test_pred_long, x = 'Model', y = 'MSE', color = 'Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc098ce",
   "metadata": {},
   "source": [
    "## Impute Missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90a4e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out a log of the enviroments imputed\n",
    "log_imputed_envs(\n",
    "    df = meta,\n",
    "    df_name = 'meta',\n",
    "    col = 'Date_Planted'\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb278b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get scaling to use on predictions\n",
    "def temp_fcn(test_this_year):\n",
    "    demo = df_prep()\n",
    "    demo.get_train_test_Envs(df = meta, \n",
    "                             holdout_years = [], \n",
    "                             test_year =     [test_this_year] )\n",
    "    demo.add_ys(df_data = df_date_to_datetime(df = meta, cols = ['Date_Planted']),\n",
    "                add_cols = ['Date_Planted'])\n",
    "    demo.add_xs(df_data = wthr, \n",
    "                drop_cols = ['Year', 'Date', 'DOY'])\n",
    "    demo.isolate_missing_y()\n",
    "    demo.prep_idx_y()\n",
    "    out = demo.mk_scale_dict(\n",
    "        scale_cols = ['Date_Planted'],\n",
    "        return_cs_dict = True,\n",
    "        store_cs_dict = False)\n",
    "    return(out)\n",
    "\n",
    "years_tested = ['2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89397b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "yHat_scaling = [pd.DataFrame(temp_fcn(test_this_year)['Date_Planted'], index = [test_this_year]) for test_this_year in years_tested]\n",
    "yHat_scaling = pd.concat(yHat_scaling).reset_index().rename(columns = {'index':'CV'})\n",
    "yHat_scaling.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c005600",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_with_mod = 'rf'\n",
    "\n",
    "file_list = [e for e in os.listdir(cache_path) if re.match('^'+impute_with_mod+'.+.csv$', e) and not re.match('.+mse.csv$', e)]\n",
    "# allow for all to further model stacking\n",
    "# for now I'm just going to use one model\n",
    "#file_list = [e for e in file_list if not  re.match('.+train_y.csv$', e)]\n",
    "\n",
    "table_list = [pd.read_csv(cache_path+e) for e in file_list]\n",
    "for i in range(len(file_list)):\n",
    "    table_list[i]['File'] = file_list[i]\n",
    "    \n",
    "yHat = pd.concat(table_list)\n",
    "\n",
    "yHat[['Model', 'Stage', 'CV', 'Set', 'Value_Known']] = yHat['File'].str.split('_', expand = True)\n",
    "yHat['CV'] = yHat['CV'].str.strip('test')\n",
    "yHat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf3f220",
   "metadata": {},
   "outputs": [],
   "source": [
    "yHat = yHat.merge(yHat_scaling)\n",
    "\n",
    "yHat['yHat'] = (yHat['yHat']*yHat['std'])+yHat['mean']\n",
    "\n",
    "yHat = yHat.loc[:, ['Env', 'Year', 'Model', 'CV', 'yHat']]\n",
    "# TODO if using multiple models and weights, those should be applied here\n",
    "\n",
    "yHat= yHat.groupby(['Env']).agg(yHat = ('yHat', np.nanmean)).reset_index()\n",
    "yHat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3439dbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values\n",
    "temp = meta\n",
    "\n",
    "temp = temp.loc[:, ['Year', 'Date_Planted']].drop_duplicates()\n",
    "temp['Date_Str'] = temp['Date_Planted'].astype(str)\n",
    "temp['DOY'] = [pd.Period(e, freq='D').day_of_year for e in list(temp['Date_Str'])]\n",
    "\n",
    "temp = meta.merge(temp).merge(yHat)\n",
    "\n",
    "px.scatter(temp, 'DOY', 'yHat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6adad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (temp.DOY.isna())\n",
    "temp.loc[mask, 'DOY'] = temp.loc[mask, 'yHat']\n",
    "temp = temp.drop(columns = ['Date_Str', 'yHat', 'Date_Planted']).rename(columns = {'DOY': 'Date_Planted'})\n",
    "\n",
    "meta = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125ca781",
   "metadata": {},
   "source": [
    "# Apply RF Imputation for missing Date_Harvested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665c6d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_col_missing(meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9d80d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup ----------------------------------------------------------------------\n",
    "trial_name = 'rf_impHarvest_test'\n",
    "\n",
    "n_trials= 200 \n",
    "n_jobs = 20\n",
    "\n",
    "def objective(trial):\n",
    "    rf_max_depth = trial.suggest_int('rf_max_depth', 2, 200, log=True)\n",
    "    rf_n_estimators = trial.suggest_int('rf_n_estimators', 2, 200, log=True)\n",
    "    rf_min_samples_split = trial.suggest_float('rf_min_samples_split', 0.01, 0.99, log=True)\n",
    "    \n",
    "    regr = RandomForestRegressor(\n",
    "        max_depth = rf_max_depth, \n",
    "        n_estimators = rf_n_estimators,\n",
    "        min_samples_split = rf_min_samples_split\n",
    "        )\n",
    "    \n",
    "    rf = regr.fit(train_x_2d, train_y_1d)\n",
    "    return (mean_squared_error(train_y_1d, rf.predict(train_x_2d), squared=False))\n",
    "\n",
    "\n",
    "reset_trial_name = trial_name\n",
    "\n",
    "if False:\n",
    "    for test_this_year in ['2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021']:\n",
    "        trial_name = reset_trial_name\n",
    "        trial_name = trial_name+test_this_year\n",
    "        print(test_this_year)\n",
    "        # Data Prep. -----------------------------------------------------------------\n",
    "        data_obj = df_prep()\n",
    "        data_obj.get_train_test_Envs(df = meta, \n",
    "                                 holdout_years = [],\n",
    "                                 test_year =     [test_this_year] )\n",
    "        data_obj.add_ys(df_data = df_date_to_datetime(df = meta, cols = ['Date_Harvested']),\n",
    "                    add_cols = ['Date_Harvested'])\n",
    "        data_obj.add_xs(df_data = wthr, \n",
    "                    drop_cols = ['Year', 'Date', 'DOY'])\n",
    "        data_obj.isolate_missing_y()\n",
    "        data_obj.prep_idx_y()\n",
    "        data_obj.mk_scale_dict(\n",
    "            scale_cols = ['Date_Harvested']+[e for e in list(wthr) if e not in ['Env', 'Year', 'Date', 'DOY']],\n",
    "            return_cs_dict = False,\n",
    "            store_cs_dict = True)\n",
    "        data_obj.apply_scaling()\n",
    "\n",
    "        train_y, train_x = data_obj.mk_arrays(\n",
    "            split = 'train',\n",
    "            obs_per_Env = 314, \n",
    "            return_2d   = False,\n",
    "            missing_ys  = False)\n",
    "\n",
    "        test_y, test_x = data_obj.mk_arrays(\n",
    "            split = 'test',\n",
    "            obs_per_Env = 314, \n",
    "            return_2d   = False,\n",
    "            missing_ys  = False)\n",
    "\n",
    "\n",
    "        train_x_2d = wthr_rank_3to2(x_3d = train_x)\n",
    "        train_y_1d = y_rank_2to1(y_2d = train_y)\n",
    "\n",
    "\n",
    "        # HPS Study ------------------------------------------------------------------\n",
    "        cache_save_name = cache_path+trial_name+'_hps.pkl'\n",
    "        if os.path.exists(cache_save_name):\n",
    "            study = pkl.load(open(cache_save_name, 'rb'))  \n",
    "        else:\n",
    "            study = optuna.create_study(direction=\"minimize\")\n",
    "            study.optimize(objective, n_trials= n_trials, n_jobs = n_jobs)\n",
    "            # save    \n",
    "            pkl.dump(study, open(cache_save_name, 'wb'))    \n",
    "\n",
    "\n",
    "        # Fit Best HPS ---------------------------------------------------------------   \n",
    "        cache_save_name = cache_path+trial_name+'_mod.pkl'\n",
    "        if os.path.exists(cache_save_name):\n",
    "            rf = pkl.load(open(cache_save_name, 'rb'))  \n",
    "        else:\n",
    "            regr = RandomForestRegressor(\n",
    "                    max_depth = study.best_trial.params['rf_max_depth'], \n",
    "                    n_estimators = study.best_trial.params['rf_n_estimators'],\n",
    "                    min_samples_split = study.best_trial.params['rf_min_samples_split']\n",
    "                    )\n",
    "            rf = regr.fit(train_x_2d, train_y_1d)\n",
    "            # save    \n",
    "            pkl.dump(rf, open(cache_save_name, 'wb'))   \n",
    "\n",
    "\n",
    "            # Eval. Best HPS -------------------------------------------------------------    \n",
    "            for i in range(4):\n",
    "                # u denotes that the true value is unknown\n",
    "                temp_label= ['train_y', 'test_y', 'train_u', 'test_u'][i]\n",
    "                if os.path.exists(cache_path+trial_name+temp_label+\".csv\"):\n",
    "                    pass\n",
    "                else:\n",
    "                    temp_data = [data_obj.train['y'], data_obj.test['y'], data_obj.train['yna'], data_obj.test['yna']][i]\n",
    "\n",
    "                    if temp_data.shape[0] != 0:\n",
    "                        # calculation step below =============================================\n",
    "                        temp_y, temp_x = data_obj.mk_arrays(                                 #\n",
    "                            split = temp_label.split('_')[0],                                #\n",
    "                            obs_per_Env = 314,                                               #\n",
    "                            return_2d   = False,                                             #\n",
    "                            missing_ys  = temp_label.split('_')[1] == 'u' )                  #\n",
    "                                                                                             #\n",
    "                        temp_y = y_rank_2to1(y_2d = temp_y)                                  #\n",
    "                        temp_yHat = rf.predict(wthr_rank_3to2(x_3d = temp_x))                #\n",
    "                        # calculation step above =============================================\n",
    "                        temp_data['yHat'] = list(temp_yHat)\n",
    "                        temp_data.to_csv(cache_path+trial_name+'_'+temp_label+\".csv\", index = False)\n",
    "\n",
    "                        if temp_label.split('_')[1] != 'u':\n",
    "                            pd.DataFrame({'MSE':[mean_squared_error(temp_y, temp_yHat)]}).to_csv(cache_path+trial_name+'_'+temp_label+\"_mse.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f824357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out a log of the enviroments imputed\n",
    "log_imputed_envs(\n",
    "    df = meta,\n",
    "    df_name = 'meta',\n",
    "    col = 'Date_Harvested'\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446d0735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_fcn(test_this_year):\n",
    "    demo = df_prep()\n",
    "    demo.get_train_test_Envs(df = meta, \n",
    "                             holdout_years = [], \n",
    "                             test_year =     [test_this_year] )\n",
    "    demo.add_ys(df_data = df_date_to_datetime(df = meta, cols = ['Date_Harvested']),\n",
    "                add_cols = ['Date_Harvested'])\n",
    "    demo.add_xs(df_data = wthr, \n",
    "                drop_cols = ['Year', 'Date', 'DOY'])\n",
    "    demo.isolate_missing_y()\n",
    "    demo.prep_idx_y()\n",
    "    out = demo.mk_scale_dict(\n",
    "        scale_cols = ['Date_Harvested'],\n",
    "        return_cs_dict = True,\n",
    "        store_cs_dict = False)\n",
    "    return(out)\n",
    "\n",
    "# get scaling to use on predictions\n",
    "years_tested = ['2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7360faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "yHat_scaling = [pd.DataFrame(temp_fcn(test_this_year)['Date_Harvested'], index = [test_this_year]) for test_this_year in years_tested]\n",
    "yHat_scaling = pd.concat(yHat_scaling).reset_index().rename(columns = {'index':'CV'})\n",
    "yHat_scaling.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9e0aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_with_mod = 'rf'\n",
    "\n",
    "file_list = [e for e in os.listdir(cache_path) if re.match('^'+impute_with_mod+'.+.csv$', e) and not re.match('.+mse.csv$', e)]\n",
    "# allow for all to further model stacking\n",
    "# for now I'm just going to use one model\n",
    "#file_list = [e for e in file_list if not  re.match('.+train_y.csv$', e)]\n",
    "\n",
    "table_list = [pd.read_csv(cache_path+e) for e in file_list]\n",
    "for i in range(len(file_list)):\n",
    "    table_list[i]['File'] = file_list[i]\n",
    "    \n",
    "yHat = pd.concat(table_list)\n",
    "\n",
    "yHat[['Model', 'Stage', 'CV', 'Set', 'Value_Known']] = yHat['File'].str.split('_', expand = True)\n",
    "yHat['CV'] = yHat['CV'].str.strip('test')\n",
    "yHat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e65c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "yHat = yHat.merge(yHat_scaling)\n",
    "\n",
    "yHat['yHat'] = (yHat['yHat']*yHat['std'])+yHat['mean']\n",
    "\n",
    "yHat = yHat.loc[:, ['Env', 'Year', 'Model', 'CV', 'yHat']]\n",
    "# TODO if using multiple models and weights, those should be applied here\n",
    "\n",
    "yHat= yHat.groupby(['Env']).agg(yHat = ('yHat', np.nanmean)).reset_index()\n",
    "yHat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85282161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values\n",
    "temp = meta\n",
    "\n",
    "temp = temp.loc[:, ['Year', 'Date_Harvested']].drop_duplicates()\n",
    "temp['Date_Str'] = temp['Date_Harvested'].astype(str)\n",
    "temp['DOY'] = [pd.Period(e, freq='D').day_of_year for e in list(temp['Date_Str'])]\n",
    "\n",
    "temp = meta.merge(temp).merge(yHat)\n",
    "\n",
    "px.scatter(temp, 'DOY', 'yHat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc1d3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (temp.DOY.isna())\n",
    "temp.loc[mask, 'DOY'] = temp.loc[mask, 'yHat']\n",
    "temp = temp.drop(columns = ['Date_Str', 'yHat', 'Date_Harvested']).rename(columns = {'DOY': 'Date_Harvested'})\n",
    "\n",
    "meta = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79856cb3",
   "metadata": {},
   "source": [
    "# Confirm no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd64347",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cols:', '\\t|', 'Min Completion %:')\n",
    "print('-----', '\\t|', '-----------------')\n",
    "for e in [meta, soil, wthr, cgmv]:\n",
    "    temp = summarize_col_missing(e)\n",
    "    print(temp.shape[0], '\\t|', min(temp.Pr_Comp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43af7d5d",
   "metadata": {},
   "source": [
    "# Save out Imputed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4c41db",
   "metadata": {},
   "source": [
    "## Make any naming tweaks for readabilty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54669e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "soil = soil.rename(columns = {\n",
    "'E Depth': 'E_Depth',\n",
    "'1:1 Soil pH': 'Soil_pH_1x1',\n",
    "'WDRF Buffer pH': 'WDRF_Buffer_pH',\n",
    "'1:1 S Salts mmho/cm': 'S_Salts_mmhopercm_1x1',\n",
    "'Texture No': 'Texture_No',\n",
    "'Organic Matter LOI %': 'Organic_Matter_LOI_Pr',\n",
    "'Nitrate-N ppm N': 'Nitrate_N_ppm_N',\n",
    "'lbs N/A': 'lbs_NperA',\n",
    "'Potassium ppm K': 'Potassium_ppm_K',\n",
    "'Sulfate-S ppm S': 'Sulfate_S_ppm_S',\n",
    "'Calcium ppm Ca': 'Calcium_ppm_Ca',\n",
    "'Magnesium ppm Mg': 'Magnesium_ppm_Mg',\n",
    "'Sodium ppm Na': 'Sodium_ppm_Na',\n",
    "'CEC/Sum of Cations me/100g': 'CECperSum_of_Cations_meper100g',\n",
    "'%H Sat': 'PrH_Sat',\n",
    "'%K Sat': 'PrK_Sat',\n",
    "'%Ca Sat': 'PrCa_Sat',\n",
    "'%Mg Sat': 'PrMg_Sat',\n",
    "'%Na Sat': 'PrNa_Sat',\n",
    "'Mehlich P-III ppm P': 'Mehlich_P_III_ppm_P',\n",
    "'% Sand': 'Pr_Sand',\n",
    "'% Silt': 'Pr_Silt',\n",
    "'% Clay': 'Pr_Clay'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0850df97",
   "metadata": {},
   "source": [
    "## Write out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43b05fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = './data/Processed/'\n",
    "if False:\n",
    "    phno.to_csv(e+'phno0.csv')\n",
    "    meta.to_csv(e+'meta0.csv')\n",
    "    soil.to_csv(e+'soil0.csv')\n",
    "    wthr.to_csv(e+'wthr0.csv')\n",
    "    cgmv.to_csv(e+'cgmv0.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0bf207",
   "metadata": {},
   "source": [
    "# Format Data for tensors, ERMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e8f6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = meta.loc[:, ['Env', 'Date_Harvested', 'Date_Planted']].drop_duplicates()\n",
    "temp['Duration'] = temp['Date_Harvested'] - temp['Date_Planted']\n",
    "max_duration = np.nanmax(temp['Duration'])\n",
    "max_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379bdbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The last harvest day is past the point I clipped the weather data (day 314)\n",
    "np.nanmax(temp['Date_Harvested'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cb84a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can't get planting to harvest for all because that would go into the following year\n",
    "np.nanmax(temp['Date_Planted'])+np.nanmax(temp['Duration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be018210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The longest growing period that I can get for all observations is:\n",
    "314-np.nanmax(temp['Date_Planted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d626e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The longest period I could get before is \n",
    "np.nanmin(temp['Date_Planted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9122d4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A full 2 months does not seem useful. If I do 36 then there will be a total of 180 observation\n",
    "144/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab67b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['Date_Planted'] = temp['Date_Planted'].astype(int)\n",
    "temp['Start_Date'] = temp['Date_Planted'] - 36\n",
    "temp['End_Date'] = temp['Date_Planted'] + 144\n",
    "\n",
    "assert 0 == (np.std(temp['End_Date'] - temp['Start_Date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe97da2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = temp.reset_index().drop(columns = 'index')\n",
    "temp['Start_Date'] = temp['Start_Date'].astype(int)\n",
    "temp['End_Date'] = temp['End_Date'].astype(int)\n",
    "temp['Date_Planted'] = temp['Date_Planted'].astype(int)\n",
    "temp['Date_Harvested'] = temp['Date_Harvested'].astype(int)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa2484b",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = temp.loc[:, ['Env','Date_Planted', \n",
    "             'Start_Date', 'End_Date']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695c5be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wthr['In_Window'] = False\n",
    "for i in temp.index:\n",
    "    Env, Start_Date, End_Date = temp.loc[i, ['Env', 'Start_Date', 'End_Date']]\n",
    "    mask = (    (wthr.Env == Env\n",
    "            ) & (wthr.DOY >= Start_Date\n",
    "            ) & (wthr.DOY < End_Date))\n",
    "    wthr.loc[mask, 'In_Window'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7291c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = wthr.In_Window\n",
    "temp = wthr.loc[mask, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee165f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_days = temp.groupby('Env').agg(Min_DOY = ('DOY', np.min)).reset_index()\n",
    "temp = temp.merge(min_days, how = \"outer\")\n",
    "\n",
    "temp['Day'] = 1+(temp['DOY'] - temp['Min_DOY'])\n",
    "\n",
    "temp = temp.drop(columns = ['Year', 'Date', 'In_Window', 'Min_DOY'])\n",
    "\n",
    "temp = temp.melt(id_vars = ['Env', 'DOY'])\n",
    "\n",
    "temp['variable'] = temp['variable'] +'_Day'+ temp['DOY'].astype(str)\n",
    "\n",
    "temp = temp.drop(columns = ['DOY'])\n",
    "\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befa7922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This here is causing problems.\n",
    "# this is going to be SLOW\n",
    "# temp.loc[(temp.Env.isin(['TXH1_2014', 'NYH3_2022'])), ].pivot(index = ['Env'], columns=['variable']).reset_index()\n",
    "temp_wide = temp.loc[:, ].pivot(index = ['Env'], columns=['variable']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b15a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = summarize_col_missing(temp_wide)\n",
    "np.min(test.loc[:, 'Pr_Comp'])\n",
    "test.sort_values('Pr_Comp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08f5daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    temp_wide.to_csv(e+'wthrWide0.csv')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,auto:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
